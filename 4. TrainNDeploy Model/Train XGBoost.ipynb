{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an Supervised XGBoost model using the data stored in feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "feature_store_session = Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we will use the feature group created during feature store lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "fd_feature_group_name = fd_feature_group_name\n",
    "\n",
    "fd_feature_group = FeatureGroup(name=fd_feature_group_name, sagemaker_session=feature_store_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Training Dataset\n",
    "\n",
    "SageMaker FeatureStore automatically builds the Glue Data Catalog for FeatureGroups (you can optionally turn it on/off while creating the FeatureGroup). In this example, we want to create one training dataset with FeatureValues from both identity and transaction FeatureGroups. This is done by utilizing the auto-built Catalog. We run an Athena query that joins the data stored in the offline store in S3 from the 2 FeatureGroups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-365792799466\n"
     ]
    }
   ],
   "source": [
    "# You can modify the following to use a bucket of your choosing\n",
    "default_s3_bucket_name = feature_store_session.default_bucket()\n",
    "prefix = 'sagemaker-featurestore-demo'\n",
    "\n",
    "print(default_s3_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactionfeaturegroup001-1629730459\n",
      "Running SELECT * FROM \"transactionfeaturegroup001-1629730459\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v26</th>\n",
       "      <th>v27</th>\n",
       "      <th>v28</th>\n",
       "      <th>amount</th>\n",
       "      <th>class</th>\n",
       "      <th>event_time</th>\n",
       "      <th>record_id</th>\n",
       "      <th>write_time</th>\n",
       "      <th>api_invocation_time</th>\n",
       "      <th>is_deleted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71501.0</td>\n",
       "      <td>-1.118315</td>\n",
       "      <td>0.656022</td>\n",
       "      <td>1.311607</td>\n",
       "      <td>-0.291451</td>\n",
       "      <td>-0.546622</td>\n",
       "      <td>-1.078459</td>\n",
       "      <td>0.294874</td>\n",
       "      <td>0.326937</td>\n",
       "      <td>0.034244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.998707</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>0.170155</td>\n",
       "      <td>62.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>109706</td>\n",
       "      <td>2021-08-23 15:02:54.465</td>\n",
       "      <td>2021-08-23 14:57:30.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>-1.169422</td>\n",
       "      <td>1.158314</td>\n",
       "      <td>1.406800</td>\n",
       "      <td>0.860189</td>\n",
       "      <td>-0.103810</td>\n",
       "      <td>0.122035</td>\n",
       "      <td>0.264451</td>\n",
       "      <td>-0.108767</td>\n",
       "      <td>-0.181977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>-0.810101</td>\n",
       "      <td>0.234957</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>53</td>\n",
       "      <td>2021-08-23 15:02:54.465</td>\n",
       "      <td>2021-08-23 14:57:30.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.0</td>\n",
       "      <td>-0.653445</td>\n",
       "      <td>0.160225</td>\n",
       "      <td>1.592256</td>\n",
       "      <td>1.296832</td>\n",
       "      <td>0.997175</td>\n",
       "      <td>-0.343000</td>\n",
       "      <td>0.469937</td>\n",
       "      <td>-0.132470</td>\n",
       "      <td>-0.197794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.348637</td>\n",
       "      <td>0.011238</td>\n",
       "      <td>-0.049478</td>\n",
       "      <td>19.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>98</td>\n",
       "      <td>2021-08-23 15:02:54.465</td>\n",
       "      <td>2021-08-23 14:57:31.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71544.0</td>\n",
       "      <td>-0.385981</td>\n",
       "      <td>0.758631</td>\n",
       "      <td>1.399149</td>\n",
       "      <td>0.824479</td>\n",
       "      <td>0.138338</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>0.832129</td>\n",
       "      <td>0.068134</td>\n",
       "      <td>-0.241392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.353056</td>\n",
       "      <td>0.048941</td>\n",
       "      <td>0.023557</td>\n",
       "      <td>49.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>109801</td>\n",
       "      <td>2021-08-23 15:02:54.465</td>\n",
       "      <td>2021-08-23 14:57:31.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71549.0</td>\n",
       "      <td>1.130942</td>\n",
       "      <td>-0.932415</td>\n",
       "      <td>0.111734</td>\n",
       "      <td>-0.653126</td>\n",
       "      <td>-1.010384</td>\n",
       "      <td>-0.476458</td>\n",
       "      <td>-0.397194</td>\n",
       "      <td>-0.111684</td>\n",
       "      <td>-1.077497</td>\n",
       "      <td>...</td>\n",
       "      <td>1.105381</td>\n",
       "      <td>-0.084372</td>\n",
       "      <td>0.015294</td>\n",
       "      <td>135.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>109811</td>\n",
       "      <td>2021-08-23 15:02:54.465</td>\n",
       "      <td>2021-08-23 14:57:31.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164529</th>\n",
       "      <td>66446.0</td>\n",
       "      <td>-1.346030</td>\n",
       "      <td>1.326632</td>\n",
       "      <td>1.257581</td>\n",
       "      <td>1.074451</td>\n",
       "      <td>0.156222</td>\n",
       "      <td>0.695528</td>\n",
       "      <td>0.190739</td>\n",
       "      <td>0.629163</td>\n",
       "      <td>-0.394167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247330</td>\n",
       "      <td>0.360396</td>\n",
       "      <td>0.127730</td>\n",
       "      <td>36.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>97934</td>\n",
       "      <td>2021-08-23 15:07:24.144</td>\n",
       "      <td>2021-08-23 15:07:21.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164530</th>\n",
       "      <td>41536.0</td>\n",
       "      <td>-2.385380</td>\n",
       "      <td>2.399992</td>\n",
       "      <td>-0.265523</td>\n",
       "      <td>-0.454264</td>\n",
       "      <td>-0.016643</td>\n",
       "      <td>1.033515</td>\n",
       "      <td>-1.321362</td>\n",
       "      <td>-1.250268</td>\n",
       "      <td>-1.844194</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039001</td>\n",
       "      <td>-0.637531</td>\n",
       "      <td>-0.142065</td>\n",
       "      <td>22.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>43515</td>\n",
       "      <td>2021-08-23 15:07:24.144</td>\n",
       "      <td>2021-08-23 15:07:22.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164531</th>\n",
       "      <td>66473.0</td>\n",
       "      <td>-1.565561</td>\n",
       "      <td>1.549571</td>\n",
       "      <td>0.761251</td>\n",
       "      <td>-1.500812</td>\n",
       "      <td>-0.004349</td>\n",
       "      <td>0.250275</td>\n",
       "      <td>-0.958032</td>\n",
       "      <td>-2.090346</td>\n",
       "      <td>-0.481178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.908449</td>\n",
       "      <td>0.030681</td>\n",
       "      <td>0.072758</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>97994</td>\n",
       "      <td>2021-08-23 15:07:24.144</td>\n",
       "      <td>2021-08-23 15:07:22.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164532</th>\n",
       "      <td>66476.0</td>\n",
       "      <td>-4.305401</td>\n",
       "      <td>2.990352</td>\n",
       "      <td>-0.106128</td>\n",
       "      <td>-0.227977</td>\n",
       "      <td>-1.293100</td>\n",
       "      <td>-0.422289</td>\n",
       "      <td>-0.245509</td>\n",
       "      <td>0.500373</td>\n",
       "      <td>1.749776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086189</td>\n",
       "      <td>-1.519313</td>\n",
       "      <td>0.552498</td>\n",
       "      <td>13.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>98000</td>\n",
       "      <td>2021-08-23 15:07:24.144</td>\n",
       "      <td>2021-08-23 15:07:22.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164533</th>\n",
       "      <td>41539.0</td>\n",
       "      <td>-1.758484</td>\n",
       "      <td>-0.629266</td>\n",
       "      <td>-0.739957</td>\n",
       "      <td>-0.444792</td>\n",
       "      <td>3.176958</td>\n",
       "      <td>3.215642</td>\n",
       "      <td>0.177705</td>\n",
       "      <td>0.634858</td>\n",
       "      <td>0.143163</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.629403</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0.286252</td>\n",
       "      <td>169.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>43522</td>\n",
       "      <td>2021-08-23 15:07:24.144</td>\n",
       "      <td>2021-08-23 15:07:22.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164534 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           time        v1        v2        v3        v4        v5        v6  \\\n",
       "0       71501.0 -1.118315  0.656022  1.311607 -0.291451 -0.546622 -1.078459   \n",
       "1          36.0 -1.169422  1.158314  1.406800  0.860189 -0.103810  0.122035   \n",
       "2          67.0 -0.653445  0.160225  1.592256  1.296832  0.997175 -0.343000   \n",
       "3       71544.0 -0.385981  0.758631  1.399149  0.824479  0.138338 -0.000999   \n",
       "4       71549.0  1.130942 -0.932415  0.111734 -0.653126 -1.010384 -0.476458   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "164529  66446.0 -1.346030  1.326632  1.257581  1.074451  0.156222  0.695528   \n",
       "164530  41536.0 -2.385380  2.399992 -0.265523 -0.454264 -0.016643  1.033515   \n",
       "164531  66473.0 -1.565561  1.549571  0.761251 -1.500812 -0.004349  0.250275   \n",
       "164532  66476.0 -4.305401  2.990352 -0.106128 -0.227977 -1.293100 -0.422289   \n",
       "164533  41539.0 -1.758484 -0.629266 -0.739957 -0.444792  3.176958  3.215642   \n",
       "\n",
       "              v7        v8        v9  ...       v26       v27       v28  \\\n",
       "0       0.294874  0.326937  0.034244  ... -0.998707 -0.000190  0.170155   \n",
       "1       0.264451 -0.108767 -0.181977  ...  0.226700 -0.810101  0.234957   \n",
       "2       0.469937 -0.132470 -0.197794  ... -0.348637  0.011238 -0.049478   \n",
       "3       0.832129  0.068134 -0.241392  ... -0.353056  0.048941  0.023557   \n",
       "4      -0.397194 -0.111684 -1.077497  ...  1.105381 -0.084372  0.015294   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "164529  0.190739  0.629163 -0.394167  ... -0.247330  0.360396  0.127730   \n",
       "164530 -1.321362 -1.250268 -1.844194  ... -0.039001 -0.637531 -0.142065   \n",
       "164531 -0.958032 -2.090346 -0.481178  ...  0.908449  0.030681  0.072758   \n",
       "164532 -0.245509  0.500373  1.749776  ...  0.086189 -1.519313  0.552498   \n",
       "164533  0.177705  0.634858  0.143163  ... -0.629403  0.050852  0.286252   \n",
       "\n",
       "        amount  class    event_time  record_id               write_time  \\\n",
       "0        62.74    0.0  1.629730e+09     109706  2021-08-23 15:02:54.465   \n",
       "1         7.99    0.0  1.629730e+09         53  2021-08-23 15:02:54.465   \n",
       "2        19.85    0.0  1.629730e+09         98  2021-08-23 15:02:54.465   \n",
       "3        49.19    0.0  1.629730e+09     109801  2021-08-23 15:02:54.465   \n",
       "4       135.00    0.0  1.629730e+09     109811  2021-08-23 15:02:54.465   \n",
       "...        ...    ...           ...        ...                      ...   \n",
       "164529   36.00    0.0  1.629730e+09      97934  2021-08-23 15:07:24.144   \n",
       "164530   22.00    0.0  1.629730e+09      43515  2021-08-23 15:07:24.144   \n",
       "164531    0.54    0.0  1.629730e+09      97994  2021-08-23 15:07:24.144   \n",
       "164532   13.54    0.0  1.629730e+09      98000  2021-08-23 15:07:24.144   \n",
       "164533  169.00    0.0  1.629730e+09      43522  2021-08-23 15:07:24.144   \n",
       "\n",
       "            api_invocation_time  is_deleted  \n",
       "0       2021-08-23 14:57:30.000       False  \n",
       "1       2021-08-23 14:57:30.000       False  \n",
       "2       2021-08-23 14:57:31.000       False  \n",
       "3       2021-08-23 14:57:31.000       False  \n",
       "4       2021-08-23 14:57:31.000       False  \n",
       "...                         ...         ...  \n",
       "164529  2021-08-23 15:07:21.000       False  \n",
       "164530  2021-08-23 15:07:22.000       False  \n",
       "164531  2021-08-23 15:07:22.000       False  \n",
       "164532  2021-08-23 15:07:22.000       False  \n",
       "164533  2021-08-23 15:07:22.000       False  \n",
       "\n",
       "[164534 rows x 36 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_query = fd_feature_group.athena_query()\n",
    "\n",
    "transaction_table = transaction_query.table_name\n",
    "\n",
    "print(transaction_table)\n",
    "\n",
    "query_string = 'SELECT * FROM \"'+transaction_table+'\"'\n",
    "print('Running ' + query_string)\n",
    "\n",
    "# run Athena query. The output is loaded to a Pandas dataframe.\n",
    "#dataset = pd.DataFrame()\n",
    "transaction_query.run(query_string=query_string, output_location='s3://'+default_s3_bucket_name+'/'+prefix+'/query_results/')\n",
    "transaction_query.wait()\n",
    "dataset = transaction_query.as_dataframe()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select useful columns for training with target column as the first.\n",
    "dataset = dataset[['time', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10',\n",
    "       'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19', 'v20',\n",
    "       'v21', 'v22', 'v23', 'v24', 'v25', 'v26', 'v27', 'v28', 'amount',\n",
    "       'class']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v21</th>\n",
       "      <th>v22</th>\n",
       "      <th>v23</th>\n",
       "      <th>v24</th>\n",
       "      <th>v25</th>\n",
       "      <th>v26</th>\n",
       "      <th>v27</th>\n",
       "      <th>v28</th>\n",
       "      <th>amount</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71501.0</td>\n",
       "      <td>-1.118315</td>\n",
       "      <td>0.656022</td>\n",
       "      <td>1.311607</td>\n",
       "      <td>-0.291451</td>\n",
       "      <td>-0.546622</td>\n",
       "      <td>-1.078459</td>\n",
       "      <td>0.294874</td>\n",
       "      <td>0.326937</td>\n",
       "      <td>0.034244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299079</td>\n",
       "      <td>0.601609</td>\n",
       "      <td>0.233180</td>\n",
       "      <td>0.681757</td>\n",
       "      <td>-1.034985</td>\n",
       "      <td>-0.998707</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>0.170155</td>\n",
       "      <td>62.74</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>-1.169422</td>\n",
       "      <td>1.158314</td>\n",
       "      <td>1.406800</td>\n",
       "      <td>0.860189</td>\n",
       "      <td>-0.103810</td>\n",
       "      <td>0.122035</td>\n",
       "      <td>0.264451</td>\n",
       "      <td>-0.108767</td>\n",
       "      <td>-0.181977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024498</td>\n",
       "      <td>-0.120153</td>\n",
       "      <td>0.212986</td>\n",
       "      <td>0.087536</td>\n",
       "      <td>-0.946530</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>-0.810101</td>\n",
       "      <td>0.234957</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.0</td>\n",
       "      <td>-0.653445</td>\n",
       "      <td>0.160225</td>\n",
       "      <td>1.592256</td>\n",
       "      <td>1.296832</td>\n",
       "      <td>0.997175</td>\n",
       "      <td>-0.343000</td>\n",
       "      <td>0.469937</td>\n",
       "      <td>-0.132470</td>\n",
       "      <td>-0.197794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038363</td>\n",
       "      <td>0.336449</td>\n",
       "      <td>-0.014883</td>\n",
       "      <td>0.102959</td>\n",
       "      <td>-0.265322</td>\n",
       "      <td>-0.348637</td>\n",
       "      <td>0.011238</td>\n",
       "      <td>-0.049478</td>\n",
       "      <td>19.85</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71544.0</td>\n",
       "      <td>-0.385981</td>\n",
       "      <td>0.758631</td>\n",
       "      <td>1.399149</td>\n",
       "      <td>0.824479</td>\n",
       "      <td>0.138338</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>0.832129</td>\n",
       "      <td>0.068134</td>\n",
       "      <td>-0.241392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008999</td>\n",
       "      <td>0.086109</td>\n",
       "      <td>-0.096526</td>\n",
       "      <td>0.074640</td>\n",
       "      <td>0.235852</td>\n",
       "      <td>-0.353056</td>\n",
       "      <td>0.048941</td>\n",
       "      <td>0.023557</td>\n",
       "      <td>49.19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71549.0</td>\n",
       "      <td>1.130942</td>\n",
       "      <td>-0.932415</td>\n",
       "      <td>0.111734</td>\n",
       "      <td>-0.653126</td>\n",
       "      <td>-1.010384</td>\n",
       "      <td>-0.476458</td>\n",
       "      <td>-0.397194</td>\n",
       "      <td>-0.111684</td>\n",
       "      <td>-1.077497</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191199</td>\n",
       "      <td>-0.361440</td>\n",
       "      <td>-0.157841</td>\n",
       "      <td>0.060984</td>\n",
       "      <td>0.260320</td>\n",
       "      <td>1.105381</td>\n",
       "      <td>-0.084372</td>\n",
       "      <td>0.015294</td>\n",
       "      <td>135.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      time        v1        v2        v3        v4        v5        v6  \\\n",
       "0  71501.0 -1.118315  0.656022  1.311607 -0.291451 -0.546622 -1.078459   \n",
       "1     36.0 -1.169422  1.158314  1.406800  0.860189 -0.103810  0.122035   \n",
       "2     67.0 -0.653445  0.160225  1.592256  1.296832  0.997175 -0.343000   \n",
       "3  71544.0 -0.385981  0.758631  1.399149  0.824479  0.138338 -0.000999   \n",
       "4  71549.0  1.130942 -0.932415  0.111734 -0.653126 -1.010384 -0.476458   \n",
       "\n",
       "         v7        v8        v9  ...       v21       v22       v23       v24  \\\n",
       "0  0.294874  0.326937  0.034244  ...  0.299079  0.601609  0.233180  0.681757   \n",
       "1  0.264451 -0.108767 -0.181977  ...  0.024498 -0.120153  0.212986  0.087536   \n",
       "2  0.469937 -0.132470 -0.197794  ...  0.038363  0.336449 -0.014883  0.102959   \n",
       "3  0.832129  0.068134 -0.241392  ... -0.008999  0.086109 -0.096526  0.074640   \n",
       "4 -0.397194 -0.111684 -1.077497  ... -0.191199 -0.361440 -0.157841  0.060984   \n",
       "\n",
       "        v25       v26       v27       v28  amount  class  \n",
       "0 -1.034985 -0.998707 -0.000190  0.170155   62.74    0.0  \n",
       "1 -0.946530  0.226700 -0.810101  0.234957    7.99    0.0  \n",
       "2 -0.265322 -0.348637  0.011238 -0.049478   19.85    0.0  \n",
       "3  0.235852 -0.353056  0.048941  0.023557   49.19    0.0  \n",
       "4  0.260320  1.105381 -0.084372  0.015294  135.00    0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.dropna()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class column corresponds to whether or not a transaction is fradulent. We see that the majority of data is non-fraudulant with only $492$ ($.173\\%$) of the data corresponding to fraudulant examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frauds:  358\n",
      "Number of non-frauds:  164175\n",
      "Percentage of fradulent data: 0.21758552995447722\n"
     ]
    }
   ],
   "source": [
    "nonfrauds, frauds = dataset.groupby('class').size()\n",
    "print('Number of frauds: ', frauds)\n",
    "print('Number of non-frauds: ', nonfrauds)\n",
    "print('Percentage of fradulent data:', 100.*frauds/(frauds + nonfrauds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 28 columns, $V_i$ for $i=1..28$ of anonymized features along with columns for time, amount, and class. We already know that the columns $V_i$ have been normalized to have $0$ mean and unit standard deviation as the result of a PCA. You can read more about PCA here:. \n",
    "\n",
    "Tip: For our dataset this amount of preprocessing will give us reasonable accuracy, but it's important to note that there are more preprocessing steps one can use to improve accuracy . For unbalanced data sets like ours where the positive (fraudulent) examples occur much less frequently than the negative (legitimate) examples, we may try “over-sampling” the minority dataset by generating synthetic data (read about SMOTE in Data Mining for Imbalanced Datasets: An Overview (https://link.springer.com/chapter/10.1007%2F0-387-25465-X_40) or undersampling the majority class by using ensemble methods (see http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.6858&rep=rep1&type=pdfor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = dataset.columns[:-1]\n",
    "label_column = dataset.columns[-1]\n",
    "\n",
    "features = dataset[feature_columns].values.astype('float32')\n",
    "labels = (dataset[label_column].values).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker XGBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data and Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sklearn\n",
    "import os\n",
    "from sklearn.datasets import dump_svmlight_file   \n",
    "\n",
    "buf = io.BytesIO()\n",
    "\n",
    "sklearn.datasets.dump_svmlight_file(X_train, y_train, buf)\n",
    "buf.seek(0);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded training data location: s3://sagemaker-us-east-1-365792799466/sagemaker-featurestore-demo/train/base/fraud-dataset\n",
      "Training artifacts will be uploaded to: s3://sagemaker-us-east-1-365792799466/sagemaker-featurestore-demo/output\n"
     ]
    }
   ],
   "source": [
    "key = 'fraud-dataset'\n",
    "subdir = 'base'\n",
    "boto3.resource('s3', region_name=region).Bucket(default_s3_bucket_name).Object(os.path.join(prefix, 'train', subdir, key)).upload_fileobj(buf)\n",
    "\n",
    "s3_train_data = 's3://{}/{}/train/{}/{}'.format(default_s3_bucket_name, prefix, subdir, key)\n",
    "print('Uploaded training data location: {}'.format(s3_train_data))\n",
    "\n",
    "output_location = 's3://{}/{}/output'.format(default_s3_bucket_name, prefix)\n",
    "print('Training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train\n",
    "\n",
    "Moving onto training, first we'll need to specify the locations of the XGBoost algorithm containers.\n",
    "To specify the Linear Learner algorithm, we use a utility function to obtain it's URI. A complete list of build-in algorithms is found here: https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can specify a few parameters like what type of training instances we'd like to use and how many, as well as our XGBoost hyperparameters.  A few key hyperparameters are:\n",
    "- `max_depth` controls how deep each tree within the algorithm can be built.  Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting.  There is typically some trade-off in model performance that needs to be explored between a large number of shallow trees and a smaller number of deeper trees.\n",
    "- `subsample` controls sampling of the training data.  This technique can help reduce overfitting, but setting it too low can also starve the model of data.\n",
    "- `num_round` controls the number of boosting rounds.  This is essentially the subsequent models that are trained using the residuals of previous iterations.  Again, more rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.\n",
    "- `eta` controls how aggressive each round of boosting is.  Larger values lead to more conservative boosting.\n",
    "- `gamma` controls how aggressively trees are grown.  Larger values lead to more conservative models.\n",
    "\n",
    "More detail on XGBoost's hyperparmeters can be found on their GitHub [page](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker abstracts training with Estimators. We can pass container, and all parameters to the estimator, as well as the hyperparameters for the linear learner and fit the estimator to the data in S3.\n",
    "Note: For IP protection reasons, SageMaker built-in algorithms, such as XGBoost, can't be run locally, i.e. on the same instance where this Jupyter Notebook code is running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_iam_role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role=sagemaker_iam_role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path=output_location,\n",
    "                                    sagemaker_session=session)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eval_metric='auc',\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-25 04:53:12 Starting - Starting the training job...\n",
      "2021-11-25 04:53:19 Starting - Launching requested ML instancesProfilerReport-1637815992: InProgress\n",
      "......\n",
      "2021-11-25 04:54:26 Starting - Preparing the instances for training............\n",
      "2021-11-25 04:56:43 Downloading - Downloading input data\n",
      "2021-11-25 04:56:43 Training - Downloading the training image...\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2021-11-25:04:57:05:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2021-11-25:04:57:05:INFO] Path /opt/ml/input/data/validation does not exist!\u001b[0m\n",
      "\u001b[34m[2021-11-25:04:57:05:INFO] File size need to be processed in the node: 90.89mb. Available memory size in the node: 8334.96mb\u001b[0m\n",
      "\u001b[34m[04:57:05] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[04:57:06] 148079x30 matrix with 4441392 entries loaded from /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[04:57:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.932835\u001b[0m\n",
      "\u001b[34m[04:57:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.932857\u001b[0m\n",
      "\u001b[34m[04:57:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.932876\u001b[0m\n",
      "\u001b[34m[04:57:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.932884\u001b[0m\n",
      "\u001b[34m[04:57:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.932883\u001b[0m\n",
      "\u001b[34m[04:57:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.93289\u001b[0m\n",
      "\u001b[34m[04:57:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.932905\u001b[0m\n",
      "\u001b[34m[04:57:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.932911\u001b[0m\n",
      "\u001b[34m[04:57:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.932896\u001b[0m\n",
      "\u001b[34m[04:57:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.932869\u001b[0m\n",
      "\u001b[34m[04:57:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 6 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.932867\u001b[0m\n",
      "\u001b[34m[04:57:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 2 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.934411\u001b[0m\n",
      "\u001b[34m[04:57:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.934274\u001b[0m\n",
      "\u001b[34m[04:57:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.934253\u001b[0m\n",
      "\u001b[34m[04:57:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.955557\u001b[0m\n",
      "\u001b[34m[04:57:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.955574\u001b[0m\n",
      "\u001b[34m[04:57:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.958978\u001b[0m\n",
      "\u001b[34m[04:57:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 10 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.958977\u001b[0m\n",
      "\u001b[34m[04:57:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.95901\u001b[0m\n",
      "\u001b[34m[04:57:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.966111\u001b[0m\n",
      "\u001b[34m[04:57:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.967502\u001b[0m\n",
      "\u001b[34m[04:57:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:0.968431\u001b[0m\n",
      "\u001b[34m[04:57:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 10 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:0.970021\u001b[0m\n",
      "\u001b[34m[04:57:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 6 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:0.977325\u001b[0m\n",
      "\u001b[34m[04:57:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:0.97909\u001b[0m\n",
      "\u001b[34m[04:57:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 6 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:0.979955\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:0.983606\u001b[0m\n",
      "\u001b[34m[04:57:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[04:57:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:0.983621\u001b[0m\n",
      "\u001b[34m[04:57:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:0.986958\u001b[0m\n",
      "\u001b[34m[04:57:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:0.987018\u001b[0m\n",
      "\u001b[34m[04:57:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 10 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:0.986808\u001b[0m\n",
      "\u001b[34m[04:57:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 8 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:0.986667\u001b[0m\n",
      "\u001b[34m[04:57:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:0.988015\u001b[0m\n",
      "\u001b[34m[04:57:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 10 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:0.987788\u001b[0m\n",
      "\u001b[34m[04:57:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:0.988917\u001b[0m\n",
      "\u001b[34m[04:57:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:0.990111\u001b[0m\n",
      "\u001b[34m[04:57:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 8 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:0.990726\u001b[0m\n",
      "\u001b[34m[04:57:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 8 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:0.992201\u001b[0m\n",
      "\u001b[34m[04:57:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 8 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:0.992593\u001b[0m\n",
      "\u001b[34m[04:57:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 6 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:0.992759\u001b[0m\n",
      "\u001b[34m[04:57:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 6 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:0.993308\u001b[0m\n",
      "\u001b[34m[04:57:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 6 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:0.993874\u001b[0m\n",
      "\u001b[34m[04:57:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[42]#011train-auc:0.994023\u001b[0m\n",
      "\u001b[34m[04:57:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[43]#011train-auc:0.994025\u001b[0m\n",
      "\u001b[34m[04:57:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-auc:0.993907\u001b[0m\n",
      "\u001b[34m[04:57:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 8 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[45]#011train-auc:0.99413\u001b[0m\n",
      "\u001b[34m[04:57:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 6 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[46]#011train-auc:0.994041\u001b[0m\n",
      "\u001b[34m[04:57:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-auc:0.994969\u001b[0m\n",
      "\u001b[34m[04:57:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[48]#011train-auc:0.995229\u001b[0m\n",
      "\u001b[34m[04:57:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 2 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[49]#011train-auc:0.995617\u001b[0m\n",
      "\u001b[34m[04:57:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[50]#011train-auc:0.995825\u001b[0m\n",
      "\u001b[34m[04:57:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-auc:0.996058\u001b[0m\n",
      "\u001b[34m[04:57:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[52]#011train-auc:0.996034\u001b[0m\n",
      "\u001b[34m[04:57:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[53]#011train-auc:0.996114\u001b[0m\n",
      "\u001b[34m[04:57:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[54]#011train-auc:0.996661\u001b[0m\n",
      "\u001b[34m[04:57:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 6 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[55]#011train-auc:0.996636\u001b[0m\n",
      "\u001b[34m[04:57:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-auc:0.997043\u001b[0m\n",
      "\u001b[34m[04:57:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 2 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[57]#011train-auc:0.997217\u001b[0m\n",
      "\u001b[34m[04:57:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[58]#011train-auc:0.997408\u001b[0m\n",
      "\u001b[34m[04:57:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[59]#011train-auc:0.997553\u001b[0m\n",
      "\u001b[34m[04:57:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 6 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[60]#011train-auc:0.997756\u001b[0m\n",
      "\u001b[34m[04:57:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[61]#011train-auc:0.997904\u001b[0m\n",
      "\u001b[34m[04:57:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[62]#011train-auc:0.997901\u001b[0m\n",
      "\u001b[34m[04:57:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 6 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[63]#011train-auc:0.998001\u001b[0m\n",
      "\u001b[34m[04:57:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[64]#011train-auc:0.998083\u001b[0m\n",
      "\u001b[34m[04:57:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[65]#011train-auc:0.998245\u001b[0m\n",
      "\u001b[34m[04:57:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[66]#011train-auc:0.998313\u001b[0m\n",
      "\u001b[34m[04:57:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 4 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[67]#011train-auc:0.998408\u001b[0m\n",
      "\u001b[34m[04:57:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[68]#011train-auc:0.998453\u001b[0m\n",
      "\u001b[34m[04:57:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[69]#011train-auc:0.998435\u001b[0m\n",
      "\u001b[34m[04:57:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[70]#011train-auc:0.998504\u001b[0m\n",
      "\u001b[34m[04:57:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 6 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[71]#011train-auc:0.998561\u001b[0m\n",
      "\u001b[34m[04:57:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 4 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[72]#011train-auc:0.99864\u001b[0m\n",
      "\u001b[34m[04:57:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[73]#011train-auc:0.998619\u001b[0m\n",
      "\u001b[34m[04:57:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[74]#011train-auc:0.99873\u001b[0m\n",
      "\u001b[34m[04:57:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 6 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[75]#011train-auc:0.998775\u001b[0m\n",
      "\u001b[34m[04:57:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 6 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[76]#011train-auc:0.998865\u001b[0m\n",
      "\u001b[34m[04:57:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 6 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[77]#011train-auc:0.998905\u001b[0m\n",
      "\u001b[34m[04:57:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 4 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[78]#011train-auc:0.998913\u001b[0m\n",
      "\u001b[34m[04:57:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[79]#011train-auc:0.998941\u001b[0m\n",
      "\u001b[34m[04:57:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 8 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[80]#011train-auc:0.998941\u001b[0m\n",
      "\u001b[34m[04:57:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[81]#011train-auc:0.999053\u001b[0m\n",
      "\u001b[34m[04:57:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[82]#011train-auc:0.999205\u001b[0m\n",
      "\u001b[34m[04:57:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[83]#011train-auc:0.999223\u001b[0m\n",
      "\u001b[34m[04:57:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 10 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[84]#011train-auc:0.999223\u001b[0m\n",
      "\u001b[34m[04:57:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 8 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[85]#011train-auc:0.999223\u001b[0m\n",
      "\u001b[34m[04:57:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[86]#011train-auc:0.999301\u001b[0m\n",
      "\u001b[34m[04:57:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 10 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[87]#011train-auc:0.999301\u001b[0m\n",
      "\u001b[34m[04:57:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[88]#011train-auc:0.999287\u001b[0m\n",
      "\u001b[34m[04:57:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 8 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[89]#011train-auc:0.999287\u001b[0m\n",
      "\u001b[34m[04:57:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[90]#011train-auc:0.999248\u001b[0m\n",
      "\u001b[34m[04:57:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[91]#011train-auc:0.999259\u001b[0m\n",
      "\u001b[34m[04:57:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[92]#011train-auc:0.999272\u001b[0m\n",
      "\u001b[34m[04:57:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 10 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[93]#011train-auc:0.999272\u001b[0m\n",
      "\u001b[34m[04:57:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 10 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[94]#011train-auc:0.999272\u001b[0m\n",
      "\u001b[34m[04:57:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[95]#011train-auc:0.999316\u001b[0m\n",
      "\n",
      "2021-11-25 04:57:48 Uploading - Uploading generated training model\n",
      "2021-11-25 04:57:48 Completed - Training job completed\n",
      "\u001b[34m[04:57:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 10 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[96]#011train-auc:0.999316\u001b[0m\n",
      "\u001b[34m[04:57:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[97]#011train-auc:0.999356\u001b[0m\n",
      "\u001b[34m[04:57:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 8 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[98]#011train-auc:0.999356\u001b[0m\n",
      "\u001b[34m[04:57:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 2 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[99]#011train-auc:0.999401\u001b[0m\n",
      "Training seconds: 83\n",
      "Billable seconds: 83\n"
     ]
    }
   ],
   "source": [
    "xgb.fit({'train': s3_train_data}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host XGBoost Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we deploy the estimator to and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: fraud-detection-xgb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import CSVDeserializer,CSVSerializer\n",
    "\n",
    "predictor = xgb.deploy(initial_instance_count=1,\n",
    "                       model_name=\"{}-xgb\".format(\"fraud-detection\"),\n",
    "                       endpoint_name=\"{}-xgb\".format(\"fraud-detection\"),\n",
    "                       instance_type=\"ml.c5.xlarge\",\n",
    "                       serializer=CSVSerializer(),\n",
    "                       deserializer=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Now that we have a hosted endpoint running, we can make real-time predictions from our model very easily, \n",
    "simply by making an http POST request.  But first, we'll need to setup serializers and deserializers for passing our `test_data` NumPy arrays to the model behind the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we have a large test set, we call predict on smaller batches\n",
    "import numpy as np\n",
    "\n",
    "def predict(current_predictor, data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, current_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_preds = predict(predictor, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a few measures from the scikit-learn package to evaluate the performance of our model. When dealing with an imbalanced dataset, we need to choose metrics that take into account the frequency of each class in the data.\n",
    "\n",
    "Two such metrics are the [balanced accuracy score](https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score), and [Cohen's Kappa](https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-s-kappa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.9458545772427724\n",
      "Cohen's Kappa = 0.9038964419872538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score\n",
    "\n",
    "# scikit-learn expects 0/1 predictions, so we threshold our raw predictions\n",
    "y_preds = np.where(raw_preds > 0.5, 1, 0)\n",
    "print(\"Balanced accuracy = {}\".format(balanced_accuracy_score(y_test, y_preds)))\n",
    "print(\"Cohen's Kappa = {}\".format(cohen_kappa_score(y_test, y_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see that our model performs very well in terms of both metrics, Cohen's Kappa scores above 0.8 are generally very favorable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from single-value metrics, it's also useful to look at metrics that indicate performance per class. A confusion matrix, and per-class precision, recall and f1-score can also provide more information about the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_predicted):\n",
    "\n",
    "    cm  = confusion_matrix(y_true, y_predicted)\n",
    "    # Get the per-class normalized value for each cell\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # We color each cell according to its normalized value, annotate with exact counts.\n",
    "    ax = sns.heatmap(cm_norm, annot=cm, fmt=\"d\")\n",
    "    ax.set(xticklabels=[\"non-fraud\", \"fraud\"], yticklabels=[\"non-fraud\", \"fraud\"])\n",
    "    ax.set_ylim([0,2])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Real Classes')\n",
    "    plt.xlabel('Predicted Classes')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-fraud       1.00      1.00      1.00     16417\n",
      "       fraud       0.92      0.89      0.90        37\n",
      "\n",
      "    accuracy                           1.00     16454\n",
      "   macro avg       0.96      0.95      0.95     16454\n",
      "weighted avg       1.00      1.00      1.00     16454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(\n",
    "    y_test, y_preds, target_names=['non-fraud', 'fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a baseline model using XGBoost, we can try to see if sampling techniques that are designed specifically for imbalanced problems can improve the performance of the model.\n",
    "\n",
    "For that purpose we will be using the [imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/index.html) package that works well with scikit-learn. \n",
    "\n",
    "We will be using [Sythetic Minority Over-sampling](https://arxiv.org/abs/1106.1813) (SMOTE), which oversamples the minority class by interpolating new data points between existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting imbalanced-learn==0.6.0\n",
      "  Downloading imbalanced_learn-0.6.0-py3-none-any.whl (162 kB)\n",
      "     |████████████████████████████████| 162 kB 26.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn==0.6.0) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn==0.6.0) (0.22.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn==0.6.0) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn==0.6.0) (1.20.3)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: scikit-learn==0.22.1 in /opt/conda/lib/python3.7/site-packages (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.22.1) (1.20.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.22.1) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.22.1) (1.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn==0.6.0\n",
    "!pip install scikit-learn==0.22.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that SMOTE has now balanced the two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 147758), (1.0, 147758)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(sorted(Counter(y_smote).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that this is a case of extreme oversampling of the the minority class, we went from ~0.17% to 50%. An alternative would be to use a smaller resampling ratio, such as having one minority cl\n",
    "ass sample for every `sqrt(non_fraud/fraud)` majority samples, or using more advanced resampling techniques. See the [comparison](https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/over-sampling/plot_comparison_over_sampling.html#sphx-glr-auto-examples-over-sampling-plot-comparison-over-sampling-py) provided by imbalanced-learn for more over-sampling options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we'll use the SMOTE dataset we just created and upload it to S3 for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded training data location: s3://sagemaker-us-east-1-365792799466/sagemaker-featurestore-demo/train/smote/fraud-dataset-smote\n",
      "Training artifacts will be uploaded to: s3://sagemaker-us-east-1-365792799466/sagemaker-featurestore-demo/smote-output\n"
     ]
    }
   ],
   "source": [
    "smote_buf = io.BytesIO()\n",
    "\n",
    "# Dump the SMOTE data into a buffer\n",
    "sklearn.datasets.dump_svmlight_file(X_smote, y_smote, smote_buf)\n",
    "smote_buf.seek(0);\n",
    "\n",
    "# Upload from the buffer to S3\n",
    "key = 'fraud-dataset-smote'\n",
    "subdir = 'smote'\n",
    "boto3.resource('s3', region_name=region).Bucket(default_s3_bucket_name).Object(os.path.join(prefix, 'train', subdir, key)).upload_fileobj(smote_buf)\n",
    "\n",
    "s3_smote_train_data = 's3://{}/{}/train/{}/{}'.format(default_s3_bucket_name, prefix, subdir, key)\n",
    "print('Uploaded training data location: {}'.format(s3_smote_train_data))\n",
    "\n",
    "smote_output_location = 's3://{}/{}/smote-output'.format(default_s3_bucket_name, prefix)\n",
    "print('Training artifacts will be uploaded to: {}'.format(smote_output_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role=sagemaker_iam_role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path=output_location,\n",
    "                                    sagemaker_session=session)\n",
    "smote_xgb.set_hyperparameters(max_depth=5,\n",
    "                        eval_metric='auc',\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-25 05:51:27 Starting - Starting the training job...\n",
      "2021-11-25 05:51:50 Starting - Launching requested ML instancesProfilerReport-1637819487: InProgress\n",
      "......\n",
      "2021-11-25 05:52:50 Starting - Preparing the instances for training............\n",
      "2021-11-25 05:54:51 Downloading - Downloading input data...\n",
      "2021-11-25 05:55:20 Training - Downloading the training image...\n",
      "2021-11-25 05:55:55 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2021-11-25:05:55:57:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2021-11-25:05:55:57:INFO] Path /opt/ml/input/data/validation does not exist!\u001b[0m\n",
      "\u001b[34m[2021-11-25:05:55:57:INFO] File size need to be processed in the node: 181.1mb. Available memory size in the node: 8340.85mb\u001b[0m\n",
      "\u001b[34m[05:55:57] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[05:55:58] 295516x30 matrix with 8864018 entries loaded from /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[05:55:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.991127\u001b[0m\n",
      "\u001b[34m[05:55:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.992021\u001b[0m\n",
      "\u001b[34m[05:56:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.992757\u001b[0m\n",
      "\u001b[34m[05:56:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.993668\u001b[0m\n",
      "\u001b[34m[05:56:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.99559\u001b[0m\n",
      "\u001b[34m[05:56:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.996974\u001b[0m\n",
      "\u001b[34m[05:56:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.997358\u001b[0m\n",
      "\u001b[34m[05:56:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.997695\u001b[0m\n",
      "\u001b[34m[05:56:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.998108\u001b[0m\n",
      "\u001b[34m[05:56:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.998696\u001b[0m\n",
      "\u001b[34m[05:56:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.998789\u001b[0m\n",
      "\u001b[34m[05:56:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.999049\u001b[0m\n",
      "\u001b[34m[05:56:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.99917\u001b[0m\n",
      "\u001b[34m[05:56:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.999289\u001b[0m\n",
      "\u001b[34m[05:56:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.999325\u001b[0m\n",
      "\u001b[34m[05:56:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.99936\u001b[0m\n",
      "\u001b[34m[05:56:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.999431\u001b[0m\n",
      "\u001b[34m[05:56:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.999544\u001b[0m\n",
      "\u001b[34m[05:56:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.999588\u001b[0m\n",
      "\u001b[34m[05:56:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.999601\u001b[0m\n",
      "\u001b[34m[05:56:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.999647\u001b[0m\n",
      "\u001b[34m[05:56:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:0.999776\u001b[0m\n",
      "\u001b[34m[05:56:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:0.999801\u001b[0m\n",
      "\u001b[34m[05:56:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:0.999808\u001b[0m\n",
      "\u001b[34m[05:56:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:0.999828\u001b[0m\n",
      "\u001b[34m[05:56:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:0.99985\u001b[0m\n",
      "\u001b[34m[05:56:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:0.999857\u001b[0m\n",
      "\u001b[34m[05:56:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:0.99987\u001b[0m\n",
      "\u001b[34m[05:56:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:0.99989\u001b[0m\n",
      "\u001b[34m[05:56:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:0.999894\u001b[0m\n",
      "\u001b[34m[05:56:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:0.9999\u001b[0m\n",
      "\u001b[34m[05:56:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:0.999905\u001b[0m\n",
      "\u001b[34m[05:56:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:0.99992\u001b[0m\n",
      "\u001b[34m[05:56:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:0.999923\u001b[0m\n",
      "\u001b[34m[05:56:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:0.999927\u001b[0m\n",
      "\u001b[34m[05:56:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:0.99993\u001b[0m\n",
      "\u001b[34m[05:56:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:0.99994\u001b[0m\n",
      "\u001b[34m[05:56:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:0.999942\u001b[0m\n",
      "\u001b[34m[05:56:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:0.999947\u001b[0m\n",
      "\u001b[34m[05:56:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:0.999949\u001b[0m\n",
      "\u001b[34m[05:56:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:0.99995\u001b[0m\n",
      "\u001b[34m[05:56:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:0.999951\u001b[0m\n",
      "\u001b[34m[05:56:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-auc:0.999954\u001b[0m\n",
      "\u001b[34m[05:56:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-auc:0.99996\u001b[0m\n",
      "\u001b[34m[05:56:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-auc:0.999962\u001b[0m\n",
      "\u001b[34m[05:56:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-auc:0.999965\u001b[0m\n",
      "\u001b[34m[05:56:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-auc:0.999965\u001b[0m\n",
      "\u001b[34m[05:56:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-auc:0.999966\u001b[0m\n",
      "\u001b[34m[05:56:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-auc:0.999967\u001b[0m\n",
      "\u001b[34m[05:56:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-auc:0.99997\u001b[0m\n",
      "\u001b[34m[05:56:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[50]#011train-auc:0.999969\u001b[0m\n",
      "\u001b[34m[05:56:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-auc:0.999971\u001b[0m\n",
      "\u001b[34m[05:56:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[52]#011train-auc:0.999973\u001b[0m\n",
      "\u001b[34m[05:56:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[53]#011train-auc:0.999976\u001b[0m\n",
      "\u001b[34m[05:56:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[54]#011train-auc:0.999977\u001b[0m\n",
      "\u001b[34m[05:56:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[55]#011train-auc:0.99998\u001b[0m\n",
      "\u001b[34m[05:56:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-auc:0.999981\u001b[0m\n",
      "\u001b[34m[05:56:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[57]#011train-auc:0.999982\u001b[0m\n",
      "\u001b[34m[05:56:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[58]#011train-auc:0.999982\u001b[0m\n",
      "\u001b[34m[05:56:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[59]#011train-auc:0.999983\u001b[0m\n",
      "\u001b[34m[05:56:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[60]#011train-auc:0.999984\u001b[0m\n",
      "\u001b[34m[05:56:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[61]#011train-auc:0.999985\u001b[0m\n",
      "\u001b[34m[05:56:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[62]#011train-auc:0.999987\u001b[0m\n",
      "\u001b[34m[05:56:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[63]#011train-auc:0.999987\u001b[0m\n",
      "\u001b[34m[05:56:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[64]#011train-auc:0.999988\u001b[0m\n",
      "\u001b[34m[05:56:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[65]#011train-auc:0.999988\u001b[0m\n",
      "\u001b[34m[05:56:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[66]#011train-auc:0.999989\u001b[0m\n",
      "\u001b[34m[05:56:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[67]#011train-auc:0.99999\u001b[0m\n",
      "\u001b[34m[05:56:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[68]#011train-auc:0.99999\u001b[0m\n",
      "\u001b[34m[05:56:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[69]#011train-auc:0.999991\u001b[0m\n",
      "\u001b[34m[05:56:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[70]#011train-auc:0.999992\u001b[0m\n",
      "\u001b[34m[05:56:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[71]#011train-auc:0.999992\u001b[0m\n",
      "\u001b[34m[05:56:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[72]#011train-auc:0.999993\u001b[0m\n",
      "\u001b[34m[05:56:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[73]#011train-auc:0.999994\u001b[0m\n",
      "\u001b[34m[05:56:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[74]#011train-auc:0.999995\u001b[0m\n",
      "\u001b[34m[05:56:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[75]#011train-auc:0.999995\u001b[0m\n",
      "\u001b[34m[05:56:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[76]#011train-auc:0.999995\u001b[0m\n",
      "\u001b[34m[05:56:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[77]#011train-auc:0.999995\u001b[0m\n",
      "\u001b[34m[05:56:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[78]#011train-auc:0.999995\u001b[0m\n",
      "\u001b[34m[05:56:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[79]#011train-auc:0.999996\u001b[0m\n",
      "\u001b[34m[05:56:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[80]#011train-auc:0.999996\u001b[0m\n",
      "\u001b[34m[05:56:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[81]#011train-auc:0.999996\u001b[0m\n",
      "\u001b[34m[05:56:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[82]#011train-auc:0.999996\u001b[0m\n",
      "\u001b[34m[05:56:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[83]#011train-auc:0.999996\u001b[0m\n",
      "\u001b[34m[05:56:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[84]#011train-auc:0.999996\u001b[0m\n",
      "\u001b[34m[05:56:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[85]#011train-auc:0.999997\u001b[0m\n",
      "\u001b[34m[05:56:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[86]#011train-auc:0.999997\u001b[0m\n",
      "\u001b[34m[05:56:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[87]#011train-auc:0.999997\u001b[0m\n",
      "\u001b[34m[05:56:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[88]#011train-auc:0.999997\u001b[0m\n",
      "\u001b[34m[05:56:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[89]#011train-auc:0.999997\u001b[0m\n",
      "\u001b[34m[05:56:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[90]#011train-auc:0.999997\u001b[0m\n",
      "\u001b[34m[05:56:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[91]#011train-auc:0.999998\u001b[0m\n",
      "\u001b[34m[05:56:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[92]#011train-auc:0.999998\u001b[0m\n",
      "\u001b[34m[05:56:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[93]#011train-auc:0.999998\u001b[0m\n",
      "\u001b[34m[05:56:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[94]#011train-auc:0.999998\u001b[0m\n",
      "\u001b[34m[05:56:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[95]#011train-auc:0.999998\u001b[0m\n",
      "\u001b[34m[05:56:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[96]#011train-auc:0.999998\u001b[0m\n",
      "\u001b[34m[05:56:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[97]#011train-auc:0.999999\u001b[0m\n",
      "\u001b[34m[05:56:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[98]#011train-auc:0.999999\u001b[0m\n",
      "\u001b[34m[05:56:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[99]#011train-auc:0.999999\u001b[0m\n",
      "\n",
      "2021-11-25 05:57:06 Uploading - Uploading generated training model\n",
      "2021-11-25 05:57:06 Completed - Training job completed\n",
      "Training seconds: 140\n",
      "Billable seconds: 140\n"
     ]
    }
   ],
   "source": [
    "smote_xgb.fit({'train': s3_smote_train_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: fraud-detection-smote-xgb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import CSVDeserializer,CSVSerializer\n",
    "\n",
    "predictor = smote_xgb.deploy(initial_instance_count=1,\n",
    "                       model_name=\"{}-xgb\".format(\"fraud-detection-smote\"),\n",
    "                       endpoint_name=\"{}-xgb\".format(\"fraud-detection-smote\"),\n",
    "                       instance_type=\"ml.c5.xlarge\",\n",
    "                       serializer=CSVSerializer(),\n",
    "                       deserializer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_raw_preds = predict(predictor, X_test)\n",
    "smote_preds = np.where(smote_raw_preds > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.9997868063592618\n",
      "Cohen's Kappa = 0.9133686042610931\n"
     ]
    }
   ],
   "source": [
    "print(\"Balanced accuracy = {}\".format(balanced_accuracy_score(y_test, smote_preds)))\n",
    "print(\"Cohen's Kappa = {}\".format(cohen_kappa_score(y_test, smote_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(y_test, smote_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-fraud       1.00      1.00      1.00     16417\n",
      "       fraud       0.84      1.00      0.91        37\n",
      "\n",
      "    accuracy                           1.00     16454\n",
      "   macro avg       0.92      1.00      0.96     16454\n",
      "weighted avg       1.00      1.00      1.00     16454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    y_test, smote_preds, target_names=['non-fraud', 'fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the randomness of XGBoost your results may vary, but overall, you should see a large increase in non-fraud cases being classified as fraud (false positives). The reason this happens is because SMOTE has oversampled the fraud class so much that it's increased its overlap in feature space with the non-fraud cases.\n",
    "Since Cohen's Kappa gives more weight to false positives than balanced accuracy does, the metric drops significantly, as does the precision and F1 score for fraud cases. However, we can bring a balance between the metrics again by adjusting our classification threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been using 0.5 as the threshold between labeling a point as fraud or not. We can try different thresholds to see if they affect the result of the classification. To evaluate we'll use the balanced accuracy and Cohen's Kappa metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.1\n",
      "Balanced accuracy = 0.999\n",
      "Cohen's Kappa = 0.621\n",
      "\n",
      "Threshold: 0.2\n",
      "Balanced accuracy = 0.999\n",
      "Cohen's Kappa = 0.804\n",
      "\n",
      "Threshold: 0.3\n",
      "Balanced accuracy = 1.000\n",
      "Cohen's Kappa = 0.840\n",
      "\n",
      "Threshold: 0.4\n",
      "Balanced accuracy = 1.000\n",
      "Cohen's Kappa = 0.881\n",
      "\n",
      "Threshold: 0.5\n",
      "Balanced accuracy = 1.000\n",
      "Cohen's Kappa = 0.913\n",
      "\n",
      "Threshold: 0.6\n",
      "Balanced accuracy = 1.000\n",
      "Cohen's Kappa = 0.937\n",
      "\n",
      "Threshold: 0.7\n",
      "Balanced accuracy = 0.986\n",
      "Cohen's Kappa = 0.923\n",
      "\n",
      "Threshold: 0.8\n",
      "Balanced accuracy = 0.973\n",
      "Cohen's Kappa = 0.909\n",
      "\n",
      "Threshold: 0.9\n",
      "Balanced accuracy = 0.959\n",
      "Cohen's Kappa = 0.919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for thres in np.linspace(0.1, 0.9, num=9):\n",
    "    smote_thres_preds = np.where(smote_raw_preds > thres, 1, 0)\n",
    "    print(\"Threshold: {:.1f}\".format(thres))\n",
    "    print(\"Balanced accuracy = {:.3f}\".format(balanced_accuracy_score(y_test, smote_thres_preds)))\n",
    "    print(\"Cohen's Kappa = {:.3f}\\n\".format(cohen_kappa_score(y_test, smote_thres_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Cohen's Kappa keeps increasing along with the threshold, without a significant loss in balanced accuracy. This adds a useful knob to our model: We can keep a low threshold if we care more about not missing any fraudulent cases, or we can increase the threshold to try to minimize the number of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with XGBoost (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared the dataset and trained our model, one thing to note is there are algorithm settings which are called \"hyperparameters\" that can dramtically affect the performance of the trained models. For example, XGBoost algorithm has dozens of hyperparameters and we need to pick the right values for those hyperparameters in order to achieve the desired model training results. Since which hyperparameter setting can lead to the best result depends on the dataset as well, it is almost impossible to pick the best hyperparameter setting without searching for it, and a good search algorithm can search for the best hyperparameter setting in an automated and effective way.\n",
    "\n",
    "We will use SageMaker hyperparameter tuning to automate the searching process effectively. Specifically, we specify a range, or a list of possible values in the case of categorical hyperparameters, for each of the hyperparameter that we plan to tune. SageMaker hyperparameter tuning will automatically launch multiple training jobs with different hyperparameter settings, evaluate results of those training jobs based on a predefined \"objective metric\", and select the hyperparameter settings for future attempts based on previous results. For each hyperparameter tuning job, we will give it a budget (max number of training jobs) and it will complete once that many training jobs have been executed.\n",
    "\n",
    "\n",
    "Now we configure the hyperparameter tuning job by using the SDK that specifies following information:\n",
    "* The Estimator to use for HPO. This we created in the earlier step for training.\n",
    "* The ranges of hyperparameters we want to tune\n",
    "* Number of training jobs to run in total and how many training jobs should be run simultaneously. More parallel jobs will finish tuning sooner, but may sacrifice accuracy. We recommend you set the parallel jobs value to less than 10% of the total number of training jobs (we'll set it higher just for this example to keep it short).\n",
    "* The objective metric that will be used to evaluate training results, in this example, we select *validation:auc* to be the objective metric and the goal is to maximize the value throughout the hyperparameter tuning process. One thing to note is the objective metric has to be among the metrics that are emitted by the algorithm during training. In this example, the built-in XGBoost algorithm emits a bunch of metrics and *validation:auc* is one of them. If you bring your own algorithm to SageMaker, then you need to make sure whatever objective metric you select, your algorithm actually emits it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will tune three hyperparameters in this examples:\n",
    "* *eta*: Step size shrinkage used in updates to prevent overfitting. After each boosting step, you can directly get the weights of new features. The eta parameter actually shrinks the feature weights to make the boosting process more conservative.  \n",
    "* *min_child_weight*: Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, the building process gives up further partitioning. In linear regression models, this simply corresponds to a minimum number of instances needed in each node. The larger the algorithm, the more conservative it is. \n",
    "* *max_depth*: Maximum depth of a tree. Increasing this value makes the model more complex and likely to be overfitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "# Define hyperparameter ranges.\n",
    "hyperparameter_ranges = {\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "    \"min_child_weight\": ContinuousParameter(1, 10),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll specify the objective metric that we'd like to tune and its definition, which includes the regular expression (Regex) needed to extract that metric from the CloudWatch logs of the training job. Since we are using built-in XGBoost algorithm here, it emits two predefined metrics: *validation:auc* and *train:auc*, and we elected to monitor *validation:auc* as you can see below. In this case, we only need to specify the metric name and do not need to provide regex. If you bring your own algorithm, your algorithm emits metrics by itself. In that case, you'll need to add a MetricDefinition object here to define the format of those metrics through regex, so that SageMaker knows how to extract those metrics from your CloudWatch logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = \"validation:auc\"\n",
    "objective_type = 'Maximize'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a `HyperparameterTuner` object, to which we pass:\n",
    "- The XGBoost estimator we created above\n",
    "- Our hyperparameter ranges\n",
    "- Objective metric name and definition\n",
    "- Tuning resource configurations such as Number of training jobs to run in total and how many training jobs can be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    xgb, objective_metric_name, hyperparameter_ranges, max_jobs=2, max_parallel_jobs=2,objective_type=objective_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................................................!\n"
     ]
    }
   ],
   "source": [
    "tuner.fit({'train': s3_train_data,'validation':s3_train_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
