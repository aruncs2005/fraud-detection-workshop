{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an Supervised XGBoost model using the data stored in feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "feature_store_session = Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "fd_feature_group_name = 'transactionfeaturegroup001'\n",
    "\n",
    "fd_feature_group = FeatureGroup(name=fd_feature_group_name, sagemaker_session=feature_store_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Training Dataset\n",
    "\n",
    "SageMaker FeatureStore automatically builds the Glue Data Catalog for FeatureGroups (you can optionally turn it on/off while creating the FeatureGroup). In this example, we want to create one training dataset with FeatureValues from both identity and transaction FeatureGroups. This is done by utilizing the auto-built Catalog. We run an Athena query that joins the data stored in the offline store in S3 from the 2 FeatureGroups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-365792799466\n"
     ]
    }
   ],
   "source": [
    "# You can modify the following to use a bucket of your choosing\n",
    "default_s3_bucket_name = feature_store_session.default_bucket()\n",
    "prefix = 'sagemaker-featurestore-demo'\n",
    "\n",
    "print(default_s3_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactionfeaturegroup001-1629730459\n",
      "Running SELECT * FROM \"transactionfeaturegroup001-1629730459\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v26</th>\n",
       "      <th>v27</th>\n",
       "      <th>v28</th>\n",
       "      <th>amount</th>\n",
       "      <th>class</th>\n",
       "      <th>event_time</th>\n",
       "      <th>record_id</th>\n",
       "      <th>write_time</th>\n",
       "      <th>api_invocation_time</th>\n",
       "      <th>is_deleted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66491.0</td>\n",
       "      <td>1.038279</td>\n",
       "      <td>-0.130291</td>\n",
       "      <td>1.033925</td>\n",
       "      <td>1.655057</td>\n",
       "      <td>-0.744910</td>\n",
       "      <td>0.105462</td>\n",
       "      <td>-0.344991</td>\n",
       "      <td>0.150649</td>\n",
       "      <td>0.861917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281158</td>\n",
       "      <td>0.067173</td>\n",
       "      <td>0.030920</td>\n",
       "      <td>38.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>98040</td>\n",
       "      <td>2021-08-23 15:12:23.044</td>\n",
       "      <td>2021-08-23 15:07:23.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97714.0</td>\n",
       "      <td>-0.388542</td>\n",
       "      <td>1.101771</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>3.513811</td>\n",
       "      <td>2.657428</td>\n",
       "      <td>5.279897</td>\n",
       "      <td>-0.563090</td>\n",
       "      <td>1.273458</td>\n",
       "      <td>-0.498999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076305</td>\n",
       "      <td>0.423914</td>\n",
       "      <td>0.229938</td>\n",
       "      <td>38.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>152982</td>\n",
       "      <td>2021-08-23 15:12:23.044</td>\n",
       "      <td>2021-08-23 15:07:23.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66500.0</td>\n",
       "      <td>-4.288288</td>\n",
       "      <td>3.183033</td>\n",
       "      <td>0.182823</td>\n",
       "      <td>-2.192241</td>\n",
       "      <td>-0.895712</td>\n",
       "      <td>-0.616880</td>\n",
       "      <td>0.288131</td>\n",
       "      <td>0.320984</td>\n",
       "      <td>2.884039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636795</td>\n",
       "      <td>0.306453</td>\n",
       "      <td>-0.042522</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>98061</td>\n",
       "      <td>2021-08-23 15:12:23.044</td>\n",
       "      <td>2021-08-23 15:07:23.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41569.0</td>\n",
       "      <td>1.188135</td>\n",
       "      <td>-0.114837</td>\n",
       "      <td>0.627819</td>\n",
       "      <td>0.867755</td>\n",
       "      <td>-0.799783</td>\n",
       "      <td>-0.678143</td>\n",
       "      <td>-0.197147</td>\n",
       "      <td>-0.011396</td>\n",
       "      <td>0.766437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183728</td>\n",
       "      <td>-0.028844</td>\n",
       "      <td>0.016836</td>\n",
       "      <td>23.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>43591</td>\n",
       "      <td>2021-08-23 15:12:23.044</td>\n",
       "      <td>2021-08-23 15:07:23.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41572.0</td>\n",
       "      <td>-2.939069</td>\n",
       "      <td>1.134611</td>\n",
       "      <td>1.489090</td>\n",
       "      <td>1.321609</td>\n",
       "      <td>-2.038069</td>\n",
       "      <td>-0.312985</td>\n",
       "      <td>-0.687178</td>\n",
       "      <td>1.097710</td>\n",
       "      <td>0.566128</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040770</td>\n",
       "      <td>0.546236</td>\n",
       "      <td>0.431051</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>43597</td>\n",
       "      <td>2021-08-23 15:12:23.044</td>\n",
       "      <td>2021-08-23 15:07:23.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164529</th>\n",
       "      <td>71458.0</td>\n",
       "      <td>1.332849</td>\n",
       "      <td>0.389198</td>\n",
       "      <td>-2.165597</td>\n",
       "      <td>-0.306873</td>\n",
       "      <td>2.641351</td>\n",
       "      <td>2.808084</td>\n",
       "      <td>-0.171627</td>\n",
       "      <td>0.683352</td>\n",
       "      <td>-0.297962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.384013</td>\n",
       "      <td>-0.028465</td>\n",
       "      <td>0.036123</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>109615</td>\n",
       "      <td>2021-08-23 15:12:23.044</td>\n",
       "      <td>2021-08-23 15:09:54.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164530</th>\n",
       "      <td>116765.0</td>\n",
       "      <td>2.015354</td>\n",
       "      <td>-1.495212</td>\n",
       "      <td>-1.866656</td>\n",
       "      <td>-1.897132</td>\n",
       "      <td>1.149671</td>\n",
       "      <td>3.702552</td>\n",
       "      <td>-1.644792</td>\n",
       "      <td>0.958487</td>\n",
       "      <td>-0.043716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052810</td>\n",
       "      <td>0.023031</td>\n",
       "      <td>-0.039031</td>\n",
       "      <td>80.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>164501</td>\n",
       "      <td>2021-08-23 15:12:23.044</td>\n",
       "      <td>2021-08-23 15:09:55.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164531</th>\n",
       "      <td>71481.0</td>\n",
       "      <td>-0.845700</td>\n",
       "      <td>1.112358</td>\n",
       "      <td>1.418798</td>\n",
       "      <td>1.293002</td>\n",
       "      <td>-0.120562</td>\n",
       "      <td>0.229485</td>\n",
       "      <td>0.110841</td>\n",
       "      <td>0.659793</td>\n",
       "      <td>-0.865973</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.218297</td>\n",
       "      <td>0.021887</td>\n",
       "      <td>0.026942</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>109663</td>\n",
       "      <td>2021-08-23 15:12:23.044</td>\n",
       "      <td>2021-08-23 15:09:55.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164532</th>\n",
       "      <td>71481.0</td>\n",
       "      <td>1.432014</td>\n",
       "      <td>-0.331789</td>\n",
       "      <td>-0.071307</td>\n",
       "      <td>-0.572398</td>\n",
       "      <td>-0.875061</td>\n",
       "      <td>-1.618266</td>\n",
       "      <td>-0.087151</td>\n",
       "      <td>-0.379435</td>\n",
       "      <td>-1.034945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101541</td>\n",
       "      <td>-0.033708</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>109665</td>\n",
       "      <td>2021-08-23 15:12:23.044</td>\n",
       "      <td>2021-08-23 15:09:55.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164533</th>\n",
       "      <td>71482.0</td>\n",
       "      <td>0.807455</td>\n",
       "      <td>-2.285814</td>\n",
       "      <td>0.427914</td>\n",
       "      <td>-1.212937</td>\n",
       "      <td>-2.044149</td>\n",
       "      <td>0.067660</td>\n",
       "      <td>-1.125666</td>\n",
       "      <td>0.153346</td>\n",
       "      <td>-1.658145</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.119412</td>\n",
       "      <td>-0.004824</td>\n",
       "      <td>0.064833</td>\n",
       "      <td>325.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.629730e+09</td>\n",
       "      <td>109668</td>\n",
       "      <td>2021-08-23 15:12:23.044</td>\n",
       "      <td>2021-08-23 15:09:55.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164534 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            time        v1        v2        v3        v4        v5        v6  \\\n",
       "0        66491.0  1.038279 -0.130291  1.033925  1.655057 -0.744910  0.105462   \n",
       "1        97714.0 -0.388542  1.101771  0.822360  3.513811  2.657428  5.279897   \n",
       "2        66500.0 -4.288288  3.183033  0.182823 -2.192241 -0.895712 -0.616880   \n",
       "3        41569.0  1.188135 -0.114837  0.627819  0.867755 -0.799783 -0.678143   \n",
       "4        41572.0 -2.939069  1.134611  1.489090  1.321609 -2.038069 -0.312985   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "164529   71458.0  1.332849  0.389198 -2.165597 -0.306873  2.641351  2.808084   \n",
       "164530  116765.0  2.015354 -1.495212 -1.866656 -1.897132  1.149671  3.702552   \n",
       "164531   71481.0 -0.845700  1.112358  1.418798  1.293002 -0.120562  0.229485   \n",
       "164532   71481.0  1.432014 -0.331789 -0.071307 -0.572398 -0.875061 -1.618266   \n",
       "164533   71482.0  0.807455 -2.285814  0.427914 -1.212937 -2.044149  0.067660   \n",
       "\n",
       "              v7        v8        v9  ...       v26       v27       v28  \\\n",
       "0      -0.344991  0.150649  0.861917  ... -0.281158  0.067173  0.030920   \n",
       "1      -0.563090  1.273458 -0.498999  ...  0.076305  0.423914  0.229938   \n",
       "2       0.288131  0.320984  2.884039  ...  0.636795  0.306453 -0.042522   \n",
       "3      -0.197147 -0.011396  0.766437  ...  0.183728 -0.028844  0.016836   \n",
       "4      -0.687178  1.097710  0.566128  ... -0.040770  0.546236  0.431051   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "164529 -0.171627  0.683352 -0.297962  ...  0.384013 -0.028465  0.036123   \n",
       "164530 -1.644792  0.958487 -0.043716  ... -0.052810  0.023031 -0.039031   \n",
       "164531  0.110841  0.659793 -0.865973  ... -0.218297  0.021887  0.026942   \n",
       "164532 -0.087151 -0.379435 -1.034945  ... -0.101541 -0.033708  0.002595   \n",
       "164533 -1.125666  0.153346 -1.658145  ... -0.119412 -0.004824  0.064833   \n",
       "\n",
       "        amount  class    event_time  record_id               write_time  \\\n",
       "0        38.00    0.0  1.629730e+09      98040  2021-08-23 15:12:23.044   \n",
       "1        38.44    0.0  1.629730e+09     152982  2021-08-23 15:12:23.044   \n",
       "2         3.79    0.0  1.629730e+09      98061  2021-08-23 15:12:23.044   \n",
       "3        23.95    0.0  1.629730e+09      43591  2021-08-23 15:12:23.044   \n",
       "4       100.00    0.0  1.629730e+09      43597  2021-08-23 15:12:23.044   \n",
       "...        ...    ...           ...        ...                      ...   \n",
       "164529    0.76    0.0  1.629730e+09     109615  2021-08-23 15:12:23.044   \n",
       "164530   80.00    0.0  1.629730e+09     164501  2021-08-23 15:12:23.044   \n",
       "164531    4.50    0.0  1.629730e+09     109663  2021-08-23 15:12:23.044   \n",
       "164532   10.00    0.0  1.629730e+09     109665  2021-08-23 15:12:23.044   \n",
       "164533  325.00    0.0  1.629730e+09     109668  2021-08-23 15:12:23.044   \n",
       "\n",
       "            api_invocation_time  is_deleted  \n",
       "0       2021-08-23 15:07:23.000       False  \n",
       "1       2021-08-23 15:07:23.000       False  \n",
       "2       2021-08-23 15:07:23.000       False  \n",
       "3       2021-08-23 15:07:23.000       False  \n",
       "4       2021-08-23 15:07:23.000       False  \n",
       "...                         ...         ...  \n",
       "164529  2021-08-23 15:09:54.000       False  \n",
       "164530  2021-08-23 15:09:55.000       False  \n",
       "164531  2021-08-23 15:09:55.000       False  \n",
       "164532  2021-08-23 15:09:55.000       False  \n",
       "164533  2021-08-23 15:09:55.000       False  \n",
       "\n",
       "[164534 rows x 36 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_query = fd_feature_group.athena_query()\n",
    "\n",
    "transaction_table = transaction_query.table_name\n",
    "\n",
    "print(transaction_table)\n",
    "\n",
    "query_string = 'SELECT * FROM \"'+transaction_table+'\"'\n",
    "print('Running ' + query_string)\n",
    "\n",
    "# run Athena query. The output is loaded to a Pandas dataframe.\n",
    "#dataset = pd.DataFrame()\n",
    "transaction_query.run(query_string=query_string, output_location='s3://'+default_s3_bucket_name+'/'+prefix+'/query_results/')\n",
    "transaction_query.wait()\n",
    "dataset = transaction_query.as_dataframe()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10',\n",
       "       'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19', 'v20',\n",
       "       'v21', 'v22', 'v23', 'v24', 'v25', 'v26', 'v27', 'v28', 'amount',\n",
       "       'class', 'event_time', 'record_id', 'write_time', 'api_invocation_time',\n",
       "       'is_deleted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select useful columns for training with target column as the first.\n",
    "dataset = dataset[['time', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10',\n",
    "       'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19', 'v20',\n",
    "       'v21', 'v22', 'v23', 'v24', 'v25', 'v26', 'v27', 'v28', 'amount',\n",
    "       'class']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v21</th>\n",
       "      <th>v22</th>\n",
       "      <th>v23</th>\n",
       "      <th>v24</th>\n",
       "      <th>v25</th>\n",
       "      <th>v26</th>\n",
       "      <th>v27</th>\n",
       "      <th>v28</th>\n",
       "      <th>amount</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66491.0</td>\n",
       "      <td>1.038279</td>\n",
       "      <td>-0.130291</td>\n",
       "      <td>1.033925</td>\n",
       "      <td>1.655057</td>\n",
       "      <td>-0.744910</td>\n",
       "      <td>0.105462</td>\n",
       "      <td>-0.344991</td>\n",
       "      <td>0.150649</td>\n",
       "      <td>0.861917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047036</td>\n",
       "      <td>0.163616</td>\n",
       "      <td>-0.055206</td>\n",
       "      <td>0.434549</td>\n",
       "      <td>0.555950</td>\n",
       "      <td>-0.281158</td>\n",
       "      <td>0.067173</td>\n",
       "      <td>0.030920</td>\n",
       "      <td>38.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97714.0</td>\n",
       "      <td>-0.388542</td>\n",
       "      <td>1.101771</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>3.513811</td>\n",
       "      <td>2.657428</td>\n",
       "      <td>5.279897</td>\n",
       "      <td>-0.563090</td>\n",
       "      <td>1.273458</td>\n",
       "      <td>-0.498999</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293353</td>\n",
       "      <td>-0.643627</td>\n",
       "      <td>0.195229</td>\n",
       "      <td>0.578783</td>\n",
       "      <td>-0.677896</td>\n",
       "      <td>0.076305</td>\n",
       "      <td>0.423914</td>\n",
       "      <td>0.229938</td>\n",
       "      <td>38.44</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66500.0</td>\n",
       "      <td>-4.288288</td>\n",
       "      <td>3.183033</td>\n",
       "      <td>0.182823</td>\n",
       "      <td>-2.192241</td>\n",
       "      <td>-0.895712</td>\n",
       "      <td>-0.616880</td>\n",
       "      <td>0.288131</td>\n",
       "      <td>0.320984</td>\n",
       "      <td>2.884039</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.474655</td>\n",
       "      <td>-0.254928</td>\n",
       "      <td>0.141980</td>\n",
       "      <td>0.157453</td>\n",
       "      <td>0.058767</td>\n",
       "      <td>0.636795</td>\n",
       "      <td>0.306453</td>\n",
       "      <td>-0.042522</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41569.0</td>\n",
       "      <td>1.188135</td>\n",
       "      <td>-0.114837</td>\n",
       "      <td>0.627819</td>\n",
       "      <td>0.867755</td>\n",
       "      <td>-0.799783</td>\n",
       "      <td>-0.678143</td>\n",
       "      <td>-0.197147</td>\n",
       "      <td>-0.011396</td>\n",
       "      <td>0.766437</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.306293</td>\n",
       "      <td>-0.830371</td>\n",
       "      <td>0.080980</td>\n",
       "      <td>0.372789</td>\n",
       "      <td>0.277700</td>\n",
       "      <td>0.183728</td>\n",
       "      <td>-0.028844</td>\n",
       "      <td>0.016836</td>\n",
       "      <td>23.95</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41572.0</td>\n",
       "      <td>-2.939069</td>\n",
       "      <td>1.134611</td>\n",
       "      <td>1.489090</td>\n",
       "      <td>1.321609</td>\n",
       "      <td>-2.038069</td>\n",
       "      <td>-0.312985</td>\n",
       "      <td>-0.687178</td>\n",
       "      <td>1.097710</td>\n",
       "      <td>0.566128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167211</td>\n",
       "      <td>0.693655</td>\n",
       "      <td>-0.303515</td>\n",
       "      <td>0.713439</td>\n",
       "      <td>0.363228</td>\n",
       "      <td>-0.040770</td>\n",
       "      <td>0.546236</td>\n",
       "      <td>0.431051</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      time        v1        v2        v3        v4        v5        v6  \\\n",
       "0  66491.0  1.038279 -0.130291  1.033925  1.655057 -0.744910  0.105462   \n",
       "1  97714.0 -0.388542  1.101771  0.822360  3.513811  2.657428  5.279897   \n",
       "2  66500.0 -4.288288  3.183033  0.182823 -2.192241 -0.895712 -0.616880   \n",
       "3  41569.0  1.188135 -0.114837  0.627819  0.867755 -0.799783 -0.678143   \n",
       "4  41572.0 -2.939069  1.134611  1.489090  1.321609 -2.038069 -0.312985   \n",
       "\n",
       "         v7        v8        v9  ...       v21       v22       v23       v24  \\\n",
       "0 -0.344991  0.150649  0.861917  ... -0.047036  0.163616 -0.055206  0.434549   \n",
       "1 -0.563090  1.273458 -0.498999  ... -0.293353 -0.643627  0.195229  0.578783   \n",
       "2  0.288131  0.320984  2.884039  ... -0.474655 -0.254928  0.141980  0.157453   \n",
       "3 -0.197147 -0.011396  0.766437  ... -0.306293 -0.830371  0.080980  0.372789   \n",
       "4 -0.687178  1.097710  0.566128  ...  0.167211  0.693655 -0.303515  0.713439   \n",
       "\n",
       "        v25       v26       v27       v28  amount  class  \n",
       "0  0.555950 -0.281158  0.067173  0.030920   38.00    0.0  \n",
       "1 -0.677896  0.076305  0.423914  0.229938   38.44    0.0  \n",
       "2  0.058767  0.636795  0.306453 -0.042522    3.79    0.0  \n",
       "3  0.277700  0.183728 -0.028844  0.016836   23.95    0.0  \n",
       "4  0.363228 -0.040770  0.546236  0.431051  100.00    0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.dropna()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class column corresponds to whether or not a transaction is fradulent. We see that the majority of data is non-fraudulant with only $492$ ($.173\\%$) of the data corresponding to fraudulant examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frauds:  358\n",
      "Number of non-frauds:  164175\n",
      "Percentage of fradulent data: 0.21758552995447722\n"
     ]
    }
   ],
   "source": [
    "nonfrauds, frauds = dataset.groupby('class').size()\n",
    "print('Number of frauds: ', frauds)\n",
    "print('Number of non-frauds: ', nonfrauds)\n",
    "print('Percentage of fradulent data:', 100.*frauds/(frauds + nonfrauds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 28 columns, $V_i$ for $i=1..28$ of anonymized features along with columns for time, amount, and class. We already know that the columns $V_i$ have been normalized to have $0$ mean and unit standard deviation as the result of a PCA. You can read more about PCA here:. \n",
    "\n",
    "Tip: For our dataset this amount of preprocessing will give us reasonable accuracy, but it's important to note that there are more preprocessing steps one can use to improve accuracy . For unbalanced data sets like ours where the positive (fraudulent) examples occur much less frequently than the negative (legitimate) examples, we may try “over-sampling” the minority dataset by generating synthetic data (read about SMOTE in Data Mining for Imbalanced Datasets: An Overview (https://link.springer.com/chapter/10.1007%2F0-387-25465-X_40) or undersampling the majority class by using ensemble methods (see http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.6858&rep=rep1&type=pdfor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = dataset.columns[:-1]\n",
    "label_column = dataset.columns[-1]\n",
    "\n",
    "features = dataset[feature_columns].values.astype('float32')\n",
    "labels = (dataset[label_column].values).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some analysis and discuss different ways we can preprocess our data. Let's discuss the way in which this data was preprocessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker XGBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data and Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sklearn\n",
    "from sklearn.datasets import dump_svmlight_file   \n",
    "\n",
    "buf = io.BytesIO()\n",
    "\n",
    "sklearn.datasets.dump_svmlight_file(X_train, y_train, buf)\n",
    "buf.seek(0);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded training data location: s3://sagemaker-us-east-1-365792799466/sagemaker-featurestore-demo/train/base/fraud-dataset\n",
      "Training artifacts will be uploaded to: s3://sagemaker-us-east-1-365792799466/sagemaker-featurestore-demo/output\n"
     ]
    }
   ],
   "source": [
    "key = 'fraud-dataset'\n",
    "subdir = 'base'\n",
    "boto3.resource('s3', region_name=region).Bucket(default_s3_bucket_name).Object(os.path.join(prefix, 'train', subdir, key)).upload_fileobj(buf)\n",
    "\n",
    "s3_train_data = 's3://{}/{}/train/{}/{}'.format(default_s3_bucket_name, prefix, subdir, key)\n",
    "print('Uploaded training data location: {}'.format(s3_train_data))\n",
    "\n",
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('Training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train\n",
    "\n",
    "Moving onto training, first we'll need to specify the locations of the XGBoost algorithm containers.\n",
    "To specify the Linear Learner algorithm, we use a utility function to obtain it's URI. A complete list of build-in algorithms is found here: https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost','latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can specify a few parameters like what type of training instances we'd like to use and how many, as well as our XGBoost hyperparameters.  A few key hyperparameters are:\n",
    "- `max_depth` controls how deep each tree within the algorithm can be built.  Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting.  There is typically some trade-off in model performance that needs to be explored between a large number of shallow trees and a smaller number of deeper trees.\n",
    "- `subsample` controls sampling of the training data.  This technique can help reduce overfitting, but setting it too low can also starve the model of data.\n",
    "- `num_round` controls the number of boosting rounds.  This is essentially the subsequent models that are trained using the residuals of previous iterations.  Again, more rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.\n",
    "- `eta` controls how aggressive each round of boosting is.  Larger values lead to more conservative boosting.\n",
    "- `gamma` controls how aggressively trees are grown.  Larger values lead to more conservative models.\n",
    "\n",
    "More detail on XGBoost's hyperparmeters can be found on their GitHub [page](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker abstracts training with Estimators. We can pass container, and all parameters to the estimator, as well as the hyperparameters for the linear learner and fit the estimator to the data in S3.\n",
    "Note: For IP protection reasons, SageMaker built-in algorithms, such as XGBoost, can't be run locally, i.e. on the same instance where this Jupyter Notebook code is running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role=sagemaker_iam_role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path=output_location,\n",
    "                                    sagemaker_session=session)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eval_metric='auc',\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-23 18:11:31 Starting - Starting the training job...\n",
      "2021-08-23 18:11:54 Starting - Launching requested ML instancesProfilerReport-1629742291: InProgress\n",
      "...\n",
      "2021-08-23 18:12:30 Starting - Preparing the instances for training.........\n",
      "2021-08-23 18:14:01 Downloading - Downloading input data...\n",
      "2021-08-23 18:14:17 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2021-08-23:18:14:39:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2021-08-23:18:14:39:INFO] Path /opt/ml/input/data/validation does not exist!\u001b[0m\n",
      "\u001b[34m[2021-08-23:18:14:39:INFO] File size need to be processed in the node: 90.89mb. Available memory size in the node: 8397.19mb\u001b[0m\n",
      "\u001b[34m[18:14:39] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[18:14:39] 148079x30 matrix with 4441384 entries loaded from /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[18:14:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.910817\u001b[0m\n",
      "\u001b[34m[18:14:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.910851\u001b[0m\n",
      "\u001b[34m[18:14:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.929488\u001b[0m\n",
      "\u001b[34m[18:14:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.929501\u001b[0m\n",
      "\u001b[34m[18:14:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.929506\u001b[0m\n",
      "\u001b[34m[18:14:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.929515\u001b[0m\n",
      "\u001b[34m[18:14:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.929519\u001b[0m\n",
      "\u001b[34m[18:14:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.929519\u001b[0m\n",
      "\u001b[34m[18:14:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.929522\u001b[0m\n",
      "\u001b[34m[18:14:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.929525\u001b[0m\n",
      "\u001b[34m[18:14:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.929529\u001b[0m\n",
      "\u001b[34m[18:14:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.92953\u001b[0m\n",
      "\u001b[34m[18:14:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.929504\u001b[0m\n",
      "\u001b[34m[18:14:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 2 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.931053\u001b[0m\n",
      "\u001b[34m[18:14:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.952291\u001b[0m\n",
      "\u001b[34m[18:14:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 8 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.952284\u001b[0m\n",
      "\u001b[34m[18:14:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.9524\u001b[0m\n",
      "\u001b[34m[18:14:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.953942\u001b[0m\n",
      "\u001b[34m[18:14:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.972453\u001b[0m\n",
      "\u001b[34m[18:14:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 12 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.972476\u001b[0m\n",
      "\u001b[34m[18:14:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 10 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.972421\u001b[0m\n",
      "\u001b[34m[18:14:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:0.973851\u001b[0m\n",
      "\u001b[34m[18:14:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 10 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:0.973164\u001b[0m\n",
      "\u001b[34m[18:14:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:0.974511\u001b[0m\n",
      "\u001b[34m[18:14:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 12 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:0.975802\u001b[0m\n",
      "\u001b[34m[18:14:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 10 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:0.975862\u001b[0m\n",
      "\u001b[34m[18:14:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 14 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:0.978428\u001b[0m\n",
      "\u001b[34m[18:14:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:0.979914\u001b[0m\n",
      "\u001b[34m[18:14:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:0.980032\u001b[0m\n",
      "\u001b[34m[18:14:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:0.985686\u001b[0m\n",
      "\u001b[34m[18:14:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:0.986341\u001b[0m\n",
      "\u001b[34m[18:14:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 10 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:0.986031\u001b[0m\n",
      "\u001b[34m[18:14:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:0.985929\u001b[0m\n",
      "\u001b[34m[18:14:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:0.989477\u001b[0m\n",
      "\u001b[34m[18:14:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:0.990868\u001b[0m\n",
      "\u001b[34m[18:14:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:0.99114\u001b[0m\n",
      "\u001b[34m[18:14:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:0.993248\u001b[0m\n",
      "\u001b[34m[18:14:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 8 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:0.993639\u001b[0m\n",
      "\u001b[34m[18:14:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 10 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:0.994093\u001b[0m\n",
      "\u001b[34m[18:14:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 8 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:0.993847\u001b[0m\n",
      "\u001b[34m[18:14:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 8 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:0.993739\u001b[0m\n",
      "\u001b[34m[18:14:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 6 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:0.9939\u001b[0m\n",
      "\n",
      "2021-08-23 18:14:55 Training - Training image download completed. Training in progress.\u001b[34m[18:14:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[42]#011train-auc:0.994119\u001b[0m\n",
      "\u001b[34m[18:14:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-auc:0.994451\u001b[0m\n",
      "\u001b[34m[18:14:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 6 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[44]#011train-auc:0.994665\u001b[0m\n",
      "\u001b[34m[18:14:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-auc:0.995113\u001b[0m\n",
      "\u001b[34m[18:14:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[46]#011train-auc:0.995458\u001b[0m\n",
      "\u001b[34m[18:14:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 6 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[47]#011train-auc:0.995659\u001b[0m\n",
      "\u001b[34m[18:14:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[48]#011train-auc:0.996376\u001b[0m\n",
      "\u001b[34m[18:14:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[49]#011train-auc:0.996601\u001b[0m\n",
      "\u001b[34m[18:14:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[50]#011train-auc:0.996939\u001b[0m\n",
      "\u001b[34m[18:14:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 6 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[51]#011train-auc:0.99719\u001b[0m\n",
      "\u001b[34m[18:14:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[52]#011train-auc:0.997261\u001b[0m\n",
      "\u001b[34m[18:14:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[53]#011train-auc:0.997583\u001b[0m\n",
      "\u001b[34m[18:14:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[54]#011train-auc:0.997626\u001b[0m\n",
      "\u001b[34m[18:14:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[55]#011train-auc:0.997603\u001b[0m\n",
      "\u001b[34m[18:14:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[56]#011train-auc:0.997761\u001b[0m\n",
      "\u001b[34m[18:14:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[57]#011train-auc:0.997884\u001b[0m\n",
      "\u001b[34m[18:14:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[58]#011train-auc:0.997991\u001b[0m\n",
      "\u001b[34m[18:14:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[59]#011train-auc:0.998095\u001b[0m\n",
      "\u001b[34m[18:14:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[60]#011train-auc:0.998111\u001b[0m\n",
      "\u001b[34m[18:14:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[61]#011train-auc:0.998386\u001b[0m\n",
      "\u001b[34m[18:14:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 8 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[62]#011train-auc:0.998426\u001b[0m\n",
      "\u001b[34m[18:14:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 6 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[63]#011train-auc:0.99837\u001b[0m\n",
      "\u001b[34m[18:14:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[64]#011train-auc:0.998406\u001b[0m\n",
      "\u001b[34m[18:14:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 8 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[65]#011train-auc:0.998501\u001b[0m\n",
      "\u001b[34m[18:14:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[66]#011train-auc:0.998503\u001b[0m\n",
      "\u001b[34m[18:14:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[67]#011train-auc:0.998492\u001b[0m\n",
      "\u001b[34m[18:14:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 6 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[68]#011train-auc:0.998603\u001b[0m\n",
      "\u001b[34m[18:14:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[69]#011train-auc:0.998686\u001b[0m\n",
      "\u001b[34m[18:14:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 6 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[70]#011train-auc:0.998661\u001b[0m\n",
      "\u001b[34m[18:14:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[71]#011train-auc:0.998671\u001b[0m\n",
      "\u001b[34m[18:14:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[72]#011train-auc:0.998816\u001b[0m\n",
      "\u001b[34m[18:14:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 12 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[73]#011train-auc:0.998816\u001b[0m\n",
      "\u001b[34m[18:14:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 6 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[74]#011train-auc:0.998854\u001b[0m\n",
      "\u001b[34m[18:14:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[75]#011train-auc:0.998939\u001b[0m\n",
      "\u001b[34m[18:14:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 10 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[76]#011train-auc:0.998939\u001b[0m\n",
      "\u001b[34m[18:14:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 6 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[77]#011train-auc:0.998985\u001b[0m\n",
      "\u001b[34m[18:14:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[78]#011train-auc:0.999026\u001b[0m\n",
      "\u001b[34m[18:14:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 6 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[79]#011train-auc:0.999005\u001b[0m\n",
      "\u001b[34m[18:14:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[80]#011train-auc:0.999069\u001b[0m\n",
      "\u001b[34m[18:14:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 6 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[81]#011train-auc:0.999056\u001b[0m\n",
      "\u001b[34m[18:14:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[82]#011train-auc:0.999038\u001b[0m\n",
      "\u001b[34m[18:15:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 12 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[83]#011train-auc:0.999038\u001b[0m\n",
      "\u001b[34m[18:15:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[84]#011train-auc:0.999002\u001b[0m\n",
      "\u001b[34m[18:15:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 10 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[85]#011train-auc:0.999002\u001b[0m\n",
      "\u001b[34m[18:15:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 10 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[86]#011train-auc:0.999002\u001b[0m\n",
      "\u001b[34m[18:15:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 4 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[87]#011train-auc:0.998997\u001b[0m\n",
      "\u001b[34m[18:15:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 10 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[88]#011train-auc:0.998997\u001b[0m\n",
      "\u001b[34m[18:15:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 6 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[89]#011train-auc:0.999005\u001b[0m\n",
      "\u001b[34m[18:15:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[90]#011train-auc:0.99909\u001b[0m\n",
      "\u001b[34m[18:15:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 2 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[91]#011train-auc:0.999149\u001b[0m\n",
      "\u001b[34m[18:15:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 8 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[92]#011train-auc:0.999164\u001b[0m\n",
      "\u001b[34m[18:15:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[93]#011train-auc:0.999153\u001b[0m\n",
      "\u001b[34m[18:15:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 10 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[94]#011train-auc:0.999153\u001b[0m\n",
      "\u001b[34m[18:15:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 10 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[95]#011train-auc:0.999153\u001b[0m\n",
      "\u001b[34m[18:15:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 12 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[96]#011train-auc:0.999153\u001b[0m\n",
      "\u001b[34m[18:15:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 6 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[97]#011train-auc:0.999161\u001b[0m\n",
      "\u001b[34m[18:15:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 4 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[98]#011train-auc:0.999204\u001b[0m\n",
      "\u001b[34m[18:15:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[99]#011train-auc:0.999169\u001b[0m\n",
      "\n",
      "2021-08-23 18:15:15 Uploading - Uploading generated training model\n",
      "2021-08-23 18:15:15 Completed - Training job completed\n",
      "Training seconds: 72\n",
      "Billable seconds: 72\n"
     ]
    }
   ],
   "source": [
    "xgb.fit({'train': s3_train_data}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host XGBoost Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we deploy the estimator to and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: fraud-detection-xgb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import CSVDeserializer,CSVSerializer\n",
    "\n",
    "predictor = xgb.deploy(initial_instance_count=1,\n",
    "                       model_name=\"{}-xgb\".format(\"fraud-detection\"),\n",
    "                       endpoint_name=\"{}-xgb\".format(\"fraud-detection\"),\n",
    "                       instance_type=\"ml.c5.xlarge\",\n",
    "                       serializer=CSVSerializer(),\n",
    "                       deserializer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.predictor import CSVDeserializer,CSVSerializer,JSONDeserializer\n",
    "xgb_predictor.serializer = CSVSerializer()\n",
    "xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Now that we have a hosted endpoint running, we can make real-time predictions from our model very easily, \n",
    "simply by making an http POST request.  But first, we'll need to setup serializers and deserializers for passing our `test_data` NumPy arrays to the model behind the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we have a large test set, we call predict on smaller batches\n",
    "def predict(current_predictor, data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, current_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_preds = predict(predictor, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a few measures from the scikit-learn package to evaluate the performance of our model. When dealing with an imbalanced dataset, we need to choose metrics that take into account the frequency of each class in the data.\n",
    "\n",
    "Two such metrics are the [balanced accuracy score](https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score), and [Cohen's Kappa](https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-s-kappa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.9473075048732944\n",
      "Cohen's Kappa = 0.9187363152955896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score\n",
    "\n",
    "# scikit-learn expects 0/1 predictions, so we threshold our raw predictions\n",
    "y_preds = np.where(raw_preds > 0.5, 1, 0)\n",
    "print(\"Balanced accuracy = {}\".format(balanced_accuracy_score(y_test, y_preds)))\n",
    "print(\"Cohen's Kappa = {}\".format(cohen_kappa_score(y_test, y_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see that our model performs very well in terms of both metrics, Cohen's Kappa scores above 0.8 are generally very favorable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from single-value metrics, it's also useful to look at metrics that indicate performance per class. A confusion matrix, and per-class precision, recall and f1-score can also provide more information about the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_predicted):\n",
    "\n",
    "    cm  = confusion_matrix(y_true, y_predicted)\n",
    "    # Get the per-class normalized value for each cell\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # We color each cell according to its normalized value, annotate with exact counts.\n",
    "    ax = sns.heatmap(cm_norm, annot=cm, fmt=\"d\")\n",
    "    ax.set(xticklabels=[\"non-fraud\", \"fraud\"], yticklabels=[\"non-fraud\", \"fraud\"])\n",
    "    ax.set_ylim([0,2])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Real Classes')\n",
    "    plt.xlabel('Predicted Classes')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-fraud       1.00      1.00      1.00     16416\n",
      "       fraud       0.94      0.89      0.92        38\n",
      "\n",
      "    accuracy                           1.00     16454\n",
      "   macro avg       0.97      0.95      0.96     16454\n",
      "weighted avg       1.00      1.00      1.00     16454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(\n",
    "    y_test, y_preds, target_names=['non-fraud', 'fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a baseline model using XGBoost, we can try to see if sampling techniques that are designed specifically for imbalanced problems can improve the performance of the model.\n",
    "\n",
    "For that purpose we will be using the [imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/index.html) package that works well with scikit-learn. We have pre-installed the package for this kernel, but if you need it for a different Jupyter kernel you can install it by running `pip install --upgrade imbalanced-learn` within the conda environment you need.\n",
    "\n",
    "We will be using [Sythetic Minority Over-sampling](https://arxiv.org/abs/1106.1813) (SMOTE), which oversamples the minority class by interpolating new data points between existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting imbalanced-learn==0.6.0\n",
      "  Downloading imbalanced_learn-0.6.0-py3-none-any.whl (162 kB)\n",
      "\u001b[K     |████████████████████████████████| 162 kB 23.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn==0.6.0) (1.21.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn==0.6.0) (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.17 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn==0.6.0) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn==0.6.0) (0.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22->imbalanced-learn==0.6.0) (2.2.0)\n",
      "Installing collected packages: imbalanced-learn\n",
      "  Attempting uninstall: imbalanced-learn\n",
      "    Found existing installation: imbalanced-learn 0.8.0\n",
      "    Uninstalling imbalanced-learn-0.8.0:\n",
      "      Successfully uninstalled imbalanced-learn-0.8.0\n",
      "Successfully installed imbalanced-learn-0.7.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting scikit-learn==0.22.1\n",
      "  Downloading scikit_learn-0.22.1-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.0 MB 23.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.22.1) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.22.1) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.22.1) (1.21.1)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "imbalanced-learn 0.7.0 requires scikit-learn>=0.23, but you have scikit-learn 0.22.1 which is incompatible.\u001b[0m\n",
      "Successfully installed scikit-learn-0.22.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn==0.6.0\n",
    "!pip install scikit-learn==0.22.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that SMOTE has now balanced the two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 147759), (1.0, 147759)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(sorted(Counter(y_smote).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that this is a case of extreme oversampling of the the minority class, we went from ~0.17% to 50%. An alternative would be to use a smaller resampling ratio, such as having one minority cl\n",
    "ass sample for every `sqrt(non_fraud/fraud)` majority samples, or using more advanced resampling techniques. See the [comparison](https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/over-sampling/plot_comparison_over_sampling.html#sphx-glr-auto-examples-over-sampling-plot-comparison-over-sampling-py) provided by imbalanced-learn for more over-sampling options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we'll use the SMOTE dataset we just created and upload it to S3 for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded training data location: s3://sagemaker-us-east-1-365792799466/sagemaker-featurestore-demo/train/smote/fraud-dataset-smote\n",
      "Training artifacts will be uploaded to: s3://sagemaker-us-east-1-365792799466/sagemaker-featurestore-demo/smote-output\n"
     ]
    }
   ],
   "source": [
    "smote_buf = io.BytesIO()\n",
    "\n",
    "# Dump the SMOTE data into a buffer\n",
    "sklearn.datasets.dump_svmlight_file(X_smote, y_smote, smote_buf)\n",
    "smote_buf.seek(0);\n",
    "\n",
    "# Upload from the buffer to S3\n",
    "key = 'fraud-dataset-smote'\n",
    "subdir = 'smote'\n",
    "boto3.resource('s3', region_name=region).Bucket(default_s3_bucket_name).Object(os.path.join(prefix, 'train', subdir, key)).upload_fileobj(smote_buf)\n",
    "\n",
    "s3_smote_train_data = 's3://{}/{}/train/{}/{}'.format(bucket, prefix, subdir, key)\n",
    "print('Uploaded training data location: {}'.format(s3_smote_train_data))\n",
    "\n",
    "smote_output_location = 's3://{}/{}/smote-output'.format(bucket, prefix)\n",
    "print('Training artifacts will be uploaded to: {}'.format(smote_output_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role=sagemaker_iam_role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path=output_location,\n",
    "                                    sagemaker_session=session)\n",
    "smote_xgb.set_hyperparameters(max_depth=5,\n",
    "                        eval_metric='auc',\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-23 18:39:38 Starting - Starting the training job...\n",
      "2021-08-23 18:40:03 Starting - Launching requested ML instancesProfilerReport-1629743978: InProgress\n",
      ".........\n",
      "2021-08-23 18:41:23 Starting - Preparing the instances for training......\n",
      "2021-08-23 18:42:38 Downloading - Downloading input data...\n",
      "2021-08-23 18:43:05 Training - Downloading the training image...\n",
      "2021-08-23 18:43:26 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2021-08-23:18:43:27:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2021-08-23:18:43:27:INFO] Path /opt/ml/input/data/validation does not exist!\u001b[0m\n",
      "\u001b[34m[2021-08-23:18:43:27:INFO] File size need to be processed in the node: 181.05mb. Available memory size in the node: 8404.46mb\u001b[0m\n",
      "\u001b[34m[18:43:27] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[18:43:28] 295518x30 matrix with 8863812 entries loaded from /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[18:43:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.992888\u001b[0m\n",
      "\u001b[34m[18:43:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.994057\u001b[0m\n",
      "\u001b[34m[18:43:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.99471\u001b[0m\n",
      "\u001b[34m[18:43:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.996419\u001b[0m\n",
      "\u001b[34m[18:43:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.996853\u001b[0m\n",
      "\u001b[34m[18:43:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.997047\u001b[0m\n",
      "\u001b[34m[18:43:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.997635\u001b[0m\n",
      "\u001b[34m[18:43:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.997814\u001b[0m\n",
      "\u001b[34m[18:43:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.997991\u001b[0m\n",
      "\u001b[34m[18:43:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.998185\u001b[0m\n",
      "\u001b[34m[18:43:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.998691\u001b[0m\n",
      "\u001b[34m[18:43:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.998967\u001b[0m\n",
      "\u001b[34m[18:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.999081\u001b[0m\n",
      "\u001b[34m[18:43:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.999182\u001b[0m\n",
      "\u001b[34m[18:43:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.999208\u001b[0m\n",
      "\u001b[34m[18:43:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.999301\u001b[0m\n",
      "\u001b[34m[18:43:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.999391\u001b[0m\n",
      "\u001b[34m[18:43:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.999425\u001b[0m\n",
      "\u001b[34m[18:43:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.999471\u001b[0m\n",
      "\u001b[34m[18:43:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.999536\u001b[0m\n",
      "\u001b[34m[18:43:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.999566\u001b[0m\n",
      "\u001b[34m[18:43:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:0.999653\u001b[0m\n",
      "\u001b[34m[18:43:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:0.999673\u001b[0m\n",
      "\u001b[34m[18:43:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:0.999698\u001b[0m\n",
      "\u001b[34m[18:43:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:0.999718\u001b[0m\n",
      "\u001b[34m[18:43:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:0.999746\u001b[0m\n",
      "\u001b[34m[18:43:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:0.999764\u001b[0m\n",
      "\u001b[34m[18:43:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:0.999799\u001b[0m\n",
      "\u001b[34m[18:43:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:0.999828\u001b[0m\n",
      "\u001b[34m[18:43:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:0.999842\u001b[0m\n",
      "\u001b[34m[18:43:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:0.999855\u001b[0m\n",
      "\u001b[34m[18:43:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:0.999862\u001b[0m\n",
      "\u001b[34m[18:43:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:0.999867\u001b[0m\n",
      "\u001b[34m[18:43:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:0.999874\u001b[0m\n",
      "\u001b[34m[18:43:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:0.999882\u001b[0m\n",
      "\u001b[34m[18:43:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:0.999884\u001b[0m\n",
      "\u001b[34m[18:43:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:0.999885\u001b[0m\n",
      "\u001b[34m[18:43:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:0.999894\u001b[0m\n",
      "\u001b[34m[18:43:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:0.999896\u001b[0m\n",
      "\u001b[34m[18:43:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:0.999901\u001b[0m\n",
      "\u001b[34m[18:43:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:0.999904\u001b[0m\n",
      "\u001b[34m[18:43:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:0.999912\u001b[0m\n",
      "\u001b[34m[18:43:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-auc:0.999912\u001b[0m\n",
      "\u001b[34m[18:43:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-auc:0.999924\u001b[0m\n",
      "\u001b[34m[18:43:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-auc:0.999931\u001b[0m\n",
      "\u001b[34m[18:43:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-auc:0.999935\u001b[0m\n",
      "\u001b[34m[18:43:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-auc:0.999937\u001b[0m\n",
      "\u001b[34m[18:43:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-auc:0.999943\u001b[0m\n",
      "\u001b[34m[18:43:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-auc:0.999947\u001b[0m\n",
      "\u001b[34m[18:43:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-auc:0.999948\u001b[0m\n",
      "\u001b[34m[18:44:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[50]#011train-auc:0.999951\u001b[0m\n",
      "\u001b[34m[18:44:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-auc:0.999955\u001b[0m\n",
      "\u001b[34m[18:44:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[52]#011train-auc:0.999959\u001b[0m\n",
      "\u001b[34m[18:44:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[53]#011train-auc:0.999964\u001b[0m\n",
      "\u001b[34m[18:44:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[54]#011train-auc:0.999965\u001b[0m\n",
      "\u001b[34m[18:44:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[55]#011train-auc:0.999966\u001b[0m\n",
      "\u001b[34m[18:44:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-auc:0.999968\u001b[0m\n",
      "\u001b[34m[18:44:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[57]#011train-auc:0.999971\u001b[0m\n",
      "\u001b[34m[18:44:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[58]#011train-auc:0.999973\u001b[0m\n",
      "\u001b[34m[18:44:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[59]#011train-auc:0.999975\u001b[0m\n",
      "\u001b[34m[18:44:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[60]#011train-auc:0.999975\u001b[0m\n",
      "\u001b[34m[18:44:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[61]#011train-auc:0.999976\u001b[0m\n",
      "\u001b[34m[18:44:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[62]#011train-auc:0.999978\u001b[0m\n",
      "\u001b[34m[18:44:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[63]#011train-auc:0.999978\u001b[0m\n",
      "\u001b[34m[18:44:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[64]#011train-auc:0.99998\u001b[0m\n",
      "\u001b[34m[18:44:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[65]#011train-auc:0.999981\u001b[0m\n",
      "\u001b[34m[18:44:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[66]#011train-auc:0.999981\u001b[0m\n",
      "\u001b[34m[18:44:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[67]#011train-auc:0.999982\u001b[0m\n",
      "\u001b[34m[18:44:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[68]#011train-auc:0.999982\u001b[0m\n",
      "\u001b[34m[18:44:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[69]#011train-auc:0.999983\u001b[0m\n",
      "\u001b[34m[18:44:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[70]#011train-auc:0.999983\u001b[0m\n",
      "\u001b[34m[18:44:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[71]#011train-auc:0.999984\u001b[0m\n",
      "\u001b[34m[18:44:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[72]#011train-auc:0.999985\u001b[0m\n",
      "\u001b[34m[18:44:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[73]#011train-auc:0.999985\u001b[0m\n",
      "\u001b[34m[18:44:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[74]#011train-auc:0.999985\u001b[0m\n",
      "\u001b[34m[18:44:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[75]#011train-auc:0.999987\u001b[0m\n",
      "\u001b[34m[18:44:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[76]#011train-auc:0.999987\u001b[0m\n",
      "\u001b[34m[18:44:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[77]#011train-auc:0.999988\u001b[0m\n",
      "\u001b[34m[18:44:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[78]#011train-auc:0.999988\u001b[0m\n",
      "\u001b[34m[18:44:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[79]#011train-auc:0.999989\u001b[0m\n",
      "\u001b[34m[18:44:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[80]#011train-auc:0.999989\u001b[0m\n",
      "\u001b[34m[18:44:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[81]#011train-auc:0.999989\u001b[0m\n",
      "\u001b[34m[18:44:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[82]#011train-auc:0.99999\u001b[0m\n",
      "\u001b[34m[18:44:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[83]#011train-auc:0.99999\u001b[0m\n",
      "\u001b[34m[18:44:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[84]#011train-auc:0.999991\u001b[0m\n",
      "\u001b[34m[18:44:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[85]#011train-auc:0.999992\u001b[0m\n",
      "\u001b[34m[18:44:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[86]#011train-auc:0.999992\u001b[0m\n",
      "\u001b[34m[18:44:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[87]#011train-auc:0.999992\u001b[0m\n",
      "\u001b[34m[18:44:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[88]#011train-auc:0.999993\u001b[0m\n",
      "\u001b[34m[18:44:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[89]#011train-auc:0.999993\u001b[0m\n",
      "\u001b[34m[18:44:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[90]#011train-auc:0.999994\u001b[0m\n",
      "\u001b[34m[18:44:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[91]#011train-auc:0.999994\u001b[0m\n",
      "\u001b[34m[18:44:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[92]#011train-auc:0.999994\u001b[0m\n",
      "\u001b[34m[18:44:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[93]#011train-auc:0.999995\u001b[0m\n",
      "\u001b[34m[18:44:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[94]#011train-auc:0.999995\u001b[0m\n",
      "\u001b[34m[18:44:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[95]#011train-auc:0.999995\u001b[0m\n",
      "\u001b[34m[18:44:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[96]#011train-auc:0.999995\u001b[0m\n",
      "\n",
      "2021-08-23 18:44:34 Uploading - Uploading generated training model\u001b[34m[18:44:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[97]#011train-auc:0.999996\u001b[0m\n",
      "\u001b[34m[18:44:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[98]#011train-auc:0.999996\u001b[0m\n",
      "\u001b[34m[18:44:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[99]#011train-auc:0.999996\u001b[0m\n",
      "\n",
      "2021-08-23 18:45:04 Completed - Training job completed\n",
      "ProfilerReport-1629743978: NoIssuesFound\n",
      "Training seconds: 124\n",
      "Billable seconds: 124\n"
     ]
    }
   ],
   "source": [
    "smote_xgb.fit({'train': s3_smote_train_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import CSVDeserializer,CSVSerializer\n",
    "\n",
    "predictor = smote_xgb.deploy(initial_instance_count=1,\n",
    "                       model_name=\"{}-xgb\".format(\"fraud-detection-smote\"),\n",
    "                       endpoint_name=\"{}-xgb\".format(\"fraud-detection-smote\"),\n",
    "                       instance_type=\"ml.c5.xlarge\",\n",
    "                       serializer=CSVSerializer(),\n",
    "                       deserializer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'smote_predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-3ef83e917e34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmote_raw_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmote_predictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msmote_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmote_raw_preds\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'smote_predictor' is not defined"
     ]
    }
   ],
   "source": [
    "smote_raw_preds = predict(smote_predictor, X_test)\n",
    "smote_preds = np.where(smote_raw_preds > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Balanced accuracy = {}\".format(balanced_accuracy_score(y_test, smote_preds)))\n",
    "print(\"Cohen's Kappa = {}\".format(cohen_kappa_score(y_test, smote_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, smote_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(\n",
    "    y_test, smote_preds, target_names=['non-fraud', 'fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the randomness of XGBoost your results may vary, but overall, you should see a large increase in non-fraud cases being classified as fraud (false positives). The reason this happens is because SMOTE has oversampled the fraud class so much that it's increased its overlap in feature space with the non-fraud cases.\n",
    "Since Cohen's Kappa gives more weight to false positives than balanced accuracy does, the metric drops significantly, as does the precision and F1 score for fraud cases. However, we can bring a balance between the metrics again by adjusting our classification threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been using 0.5 as the threshold between labeling a point as fraud or not. We can try different thresholds to see if they affect the result of the classification. To evaluate we'll use the balanced accuracy and Cohen's Kappa metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thres in np.linspace(0.1, 0.9, num=9):\n",
    "    smote_thres_preds = np.where(smote_raw_preds > thres, 1, 0)\n",
    "    print(\"Threshold: {:.1f}\".format(thres))\n",
    "    print(\"Balanced accuracy = {:.3f}\".format(balanced_accuracy_score(y_test, smote_thres_preds)))\n",
    "    print(\"Cohen's Kappa = {:.3f}\\n\".format(cohen_kappa_score(y_test, smote_thres_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Cohen's Kappa keeps increasing along with the threshold, without a significant loss in balanced accuracy. This adds a useful knob to our model: We can keep a low threshold if we care more about not missing any fraudulent cases, or we can increase the threshold to try to minimize the number of false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
